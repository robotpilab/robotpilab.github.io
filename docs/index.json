
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"","date":1757542611,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1757542611,"objectID":"8fd126c1b54805809e02bbdc6bceee71","permalink":"https://robotpilab.github.io/author/gaotian-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/gaotian-wang/","section":"authors","summary":"","tags":null,"title":"Gaotian Wang","type":"authors"},{"authors":null,"categories":null,"content":"","date":1757542611,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1757542611,"objectID":"abb5202e0fc2f345647e5f3a743e5b62","permalink":"https://robotpilab.github.io/author/kejia-ren/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/kejia-ren/","section":"authors","summary":"","tags":null,"title":"Kejia Ren","type":"authors"},{"authors":null,"categories":null,"content":"","date":1754006400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1754006400,"objectID":"f275c7d90e24ac7270118af30f79880f","permalink":"https://robotpilab.github.io/author/podshara-chanrungmaneekul/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/podshara-chanrungmaneekul/","section":"authors","summary":"","tags":null,"title":"Podshara Chanrungmaneekul","type":"authors"},{"authors":null,"categories":null,"content":"","date":1752784800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1752784800,"objectID":"df4834322c8c80f8295cc9f1ab51da79","permalink":"https://robotpilab.github.io/author/hayden-webb/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hayden-webb/","section":"authors","summary":"","tags":null,"title":"Hayden Webb","type":"authors"},{"authors":null,"categories":null,"content":"","date":1752784800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1752784800,"objectID":"7633ba684aa6acd109055fe68e051ede","permalink":"https://robotpilab.github.io/author/yiting-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yiting-chen/","section":"authors","summary":"","tags":null,"title":"Yiting Chen","type":"authors"},{"authors":null,"categories":null,"content":"","date":1723408800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1723408800,"objectID":"677a95a53264a47039d0d3d5bc8f4a7b","permalink":"https://robotpilab.github.io/author/howard-qian/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/howard-qian/","section":"authors","summary":"","tags":null,"title":"Howard Qian","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"dff5f542fedc1d7c8fbc4506d9117438","permalink":"https://robotpilab.github.io/author/ben-leebron/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ben-leebron/","section":"authors","summary":"","tags":null,"title":"Ben Leebron","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"836c93db16d6da033bdb1d1677b55f78","permalink":"https://robotpilab.github.io/author/erich-mcmillan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/erich-mcmillan/","section":"authors","summary":"","tags":null,"title":"Erich McMillan","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1c9174dc92380be02ff519647ee3b077","permalink":"https://robotpilab.github.io/author/fangming-cheng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/fangming-cheng/","section":"authors","summary":"","tags":null,"title":"Fangming Cheng","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"da99cb196019cc5857b9b3e950397ca9","permalink":"https://robotpilab.github.io/author/kaiyu-hang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/kaiyu-hang/","section":"authors","summary":"","tags":null,"title":"Kaiyu Hang","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6834dac3a267e3f4e742e3d4d6a7dd01","permalink":"https://robotpilab.github.io/author/mitchell-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mitchell-chen/","section":"authors","summary":"","tags":null,"title":"Mitchell Chen","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6eec1511a6700c818b13fd83335d3403","permalink":"https://robotpilab.github.io/author/weihang-guo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/weihang-guo/","section":"authors","summary":"","tags":null,"title":"Weihang Guo","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"91dfba8bb750d990e64c3cf09c9c3a70","permalink":"https://robotpilab.github.io/author/xiaoyu-yuan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xiaoyu-yuan/","section":"authors","summary":"","tags":null,"title":"Xiaoyu Yuan","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"24ab20a5c9e207ea81e964507ba3dc44","permalink":"https://robotpilab.github.io/author/zhuoyi-lu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhuoyi-lu/","section":"authors","summary":"","tags":null,"title":"Zhuoyi Lu","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"beab10bda44dd6e0033d79df45632d26","permalink":"https://robotpilab.github.io/author/zikang-sheng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zikang-sheng/","section":"authors","summary":"","tags":null,"title":"Zikang Sheng","type":"authors"},{"authors":["Kejia Ren","Gaotian Wang","Andrew S. Morgan","Lydia E. Kavraki","Kaiyu Hang"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1757542611,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1757542611,"objectID":"7b255f62e6b97b5f848fdb2b0c945a2d","permalink":"https://robotpilab.github.io/publication/uno_sort/","publishdate":"2025-09-10T22:16:51Z","relpermalink":"/publication/uno_sort/","section":"publication","summary":"We propose a novel object-centric planning paradigm for nonprehensile robot rearrangement manipulation. Our approach focuses on planning desired object motions, which are then realized via robot actions generated online. Experiments show that this paradigm generates more intuitive and efficient robot actions compared to traditional robot-centric approaches.","tags":["Source Themes"],"title":"Object-Centric Kinodynamic Planning for Nonprehensile Robot Rearrangement Manipulation","type":"publication"},{"authors":["Joshua T. Grace","Podshara Chanrungmaneekul","Kaiyu Hang","Aaron M. Dollar"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1754006400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1754006400,"objectID":"2135f0797ea9df929139de8a28c04612","permalink":"https://robotpilab.github.io/publication/josh2/","publishdate":"2025-08-01T00:00:00Z","relpermalink":"/publication/josh2/","section":"publication","summary":"We introduce an inverse Jacobian estimation method that separately identifies motion direction and magnitude without any prior system knowledge, enabling a unified framework for hands and arms. Experiments on the Yale Model O, Yale Stewart Hand, and a UR5e arm demonstrate real-time control with submillimeter precision.","tags":["Source Themes"],"title":"On the Role of Jacobians in Robust Manipulation","type":"publication"},{"authors":false,"categories":null,"content":"Our paper “Collision-Inclusive Manipulation Planning for Occluded Object Grasping via Compliant Robot Motions” has been accepted by IEEE Robotics and Automation Letters (RA-L).\n","date":1753826894,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1753826894,"objectID":"13f72f304af3f82216e6a5fd9a5481cd","permalink":"https://robotpilab.github.io/post/kejia-ral/","publishdate":"2025-07-29T22:08:14.077Z","relpermalink":"/post/kejia-ral/","section":"post","summary":"Our paper “Collision-Inclusive Manipulation Planning for Occluded Object Grasping via Compliant Robot Motions” has been accepted by IEEE Robotics and Automation Letters (RA-L).","tags":null,"title":"Our paper \"Collision-Inclusive Manipulation Planning for Occluded Object Grasping via Compliant Robot Motions\" has been accepted by RA-L","type":"post"},{"authors":["Podshara Chanrungmaneekul","Yiting Chen","Joshua T. Grace","Aaron M. Dollar","Kaiyu Hang"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1752784800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1752784800,"objectID":"a87a08224c411b8b99259f00640d85cc","permalink":"https://robotpilab.github.io/publication/arccalib/","publishdate":"2025-07-17T00:00:00Z","relpermalink":"/publication/arccalib/","section":"publication","summary":"ARC-Calib is a fully autonomous, model-based markerless camera-to-robot calibration framework that leverages exploratory robot motions and geometric optimization to achieve robust, generalizable calibration across diverse robots and scenarios, without requiring markers, pre-trained models, or extensive data collection.","tags":["Robotics","Computer Vision","Calibration"],"title":"ARC-Calib: Autonomous Markerless Camera-to-Robot Calibration via Exploratory Robot Motions","type":"publication"},{"authors":["Benjamin H. Leebron","Kejia Ren","Yiting Chen","Kaiyu Hang"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1752784800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1752784800,"objectID":"b3ed37ac4e0bd3f27b3b0a22b227a6f7","permalink":"https://robotpilab.github.io/publication/b4p/","publishdate":"2025-07-17T00:00:00Z","relpermalink":"/publication/b4p/","section":"publication","summary":"We propose B4P, a forest-based planning framework that simultaneously finds grasp configurations and feasible robot motions for object placement. Our bidirectional sampling-based approach builds start and goal forests to connect valid grasp-placement pairs, enabling superlinear speedup through inherent parallelism. This makes the framework scalable for redundant robot arms working in highly cluttered environments, addressing the sub-optimality introduced by traditional decoupled planning approaches.","tags":["Source Themes"],"title":"B4P: Simultaneous Grasp and Motion Planning for Object Placement via Parallelized Bidirectional Forests and Path Repair","type":"publication"},{"authors":["Howard H. Qian","Yiting Chen","Gaotian Wang","Podshara Chanrungmaneekul","Kaiyu Hang"],"categories":null,"content":"","date":1752784800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1752784800,"objectID":"bf24a168564854b57d3a2d05d09cb983","permalink":"https://robotpilab.github.io/publication/riseg2/","publishdate":"2025-07-11T20:40:00Z","relpermalink":"/publication/riseg2/","section":"publication","summary":"rt-RISeg is a real-time, model-free framework for robot interactive segmentation of unseen objects. By leveraging robot interactions and analyzing body frame-invariant features, it can segment objects without any learned model. The method achieves significantly higher accuracy than previous approaches and can also enhance the performance of vision foundation models by providing high-quality segmentation masks.","tags":["Interactive Perception","Robotics","Segmentation","Unseen Object Instance Segmentation"],"title":"rt-RISeg: Real-Time Model-Free Robot Interactive Segmentation for Active Instance-Level Object Understanding","type":"publication"},{"authors":["Hayden Webb","Podshara Chanrungmaneekul","Shenli Yuan","Kaiyu Hang"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1752784800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1752784800,"objectID":"243abb760593ad30f8f059efa74a6943","permalink":"https://robotpilab.github.io/publication/hayden/","publishdate":"2025-07-17T00:00:00Z","relpermalink":"/publication/hayden/","section":"publication","summary":"We present Roller Rings (RR), a modular robotic attachment with active surfaces that enables dexterous in-hand manipulation when worn by robot and human hands. By angling the RRs such that their motions are not co-linear, we derive a general differential motion model showing that complete manipulation skill sets can be achieved with as few as 2 RRs through non-holonomic object motions. More RRs enhance dexterity with fewer constraints. Experiments on robot and human hands validate the RRs' capability to manipulate arbitrary object shapes and provide dexterous in-hand manipulation.","tags":["Source Themes"],"title":"Wearable Roller Rings to Enable Robot Dexterous In-Hand Manipulation through Active Surfaces","type":"publication"},{"authors":["Kejia Ren","Gaotian Wang","Andrew S. Morgan","Kaiyu Hang"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1752351165,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1752351165,"objectID":"8bf9f167cdee1e455dc8c4afc96a68f9","permalink":"https://robotpilab.github.io/publication/collision-inclusive-manipulation/","publishdate":"2025-07-12T20:12:45Z","relpermalink":"/publication/collision-inclusive-manipulation/","section":"publication","summary":"We propose a collision-inclusive planning framework for occluded object grasping that allows and exploits intentional collisions through compliant robot motions, demonstrating effective manipulation in scenarios where traditional collision-free approaches are insufficient.","tags":["Robotics","Object Manipulation","Collision-inclusive Planning","Compliant Control"],"title":"Collision-inclusive Manipulation Planning for Occluded Object Grasping via Compliant Robot Motions","type":"publication"},{"authors":false,"categories":null,"content":"We have 5 papers accepted to the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)! Our contributions cover the topics of robot interactive perception, autonomous markerless calibration, combined pick-motion-and-place planning, active surface-based end-effector design, and self-identification for in-hand manipulation. Please check out the papers here: @rhttps://robotpilab.github.io/publication/!\n","date":1750896000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1750896000,"objectID":"a5ff033e4a96fc72e745a5d77aaf1dc0","permalink":"https://robotpilab.github.io/post/2025_iros/","publishdate":"2025-06-26T00:00:00Z","relpermalink":"/post/2025_iros/","section":"post","summary":"We have 5 papers accepted to the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)! Our contributions cover the topics of robot interactive perception, autonomous markerless calibration, combined pick-motion-and-place planning, active surface-based end-effector design, and self-identification for in-hand manipulation.","tags":null,"title":"5 papers accepted to IEEE/RSJ IROS 2025","type":"post"},{"authors":false,"categories":null,"content":"We are proud to announce that our esteemed lab member, Ben Leebron, has been awarded the prestigious “Distinction in Research and Creative Works” by Rice University.\nThis is a university honor for graduating students that will be granted at Commencement and will appear on recipients’ transcripts and diplomas. To apply, CS students should submit a work or body of work that illustrates creativity, self-initiation, independence, perseverance, and dedication. In all cases, we expect that students who apply for distinction in undergraduate research in computer science will have produced something that goes beyond the traditional expectations of their coursework.\nBen’s successful application for this award cited his first authorship of the “B4P: Simultaneous Grasp and Motion Planning for Object Placement via Parallelized Bidirectional Forests and Path Repair” manuscript, which was submitted to IROS 2025. We anticipate hearing about its acceptance into this prestigious conference in late June.\nThis award underscores the high-quality research being conducted in our lab and highlights Ben’s own contribution to the RobotPI lab’s success. We congratulate Ben on this well-deserved recognition and look forward to his continued effort in future works.\n","date":1746403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1746403200,"objectID":"77ae6d296fb4d167eea3a5f43fa530ff","permalink":"https://robotpilab.github.io/post/2025_ben_distinction/","publishdate":"2025-05-05T00:00:00Z","relpermalink":"/post/2025_ben_distinction/","section":"post","summary":"We are proud to announce that our esteemed lab member, Ben Leebron, has been awarded the prestigious “Distinction in Research and Creative Works” by Rice University.\nThis is a university honor for graduating students that will be granted at Commencement and will appear on recipients’ transcripts and diplomas.","tags":null,"title":"Ben Leebron has Been Awarded \"Distinction in Research and Creative Works\" by Rice University","type":"post"},{"authors":["Gaotian Wang","Kejia Ren","Andrew S. Morgan","Kaiyu Hang"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1746303165,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1746303165,"objectID":"32a93bc84840086ee42861a78af91fe4","permalink":"https://robotpilab.github.io/publication/caging/","publishdate":"2025-05-03T20:12:45Z","relpermalink":"/publication/caging/","section":"publication","summary":"We propose a novel concept called \"Caging in Time\" that enables robust object manipulation with a single end-effector under uncertainties and limited perception. This approach allows caging configurations to be formed in time, demonstrating effective open-loop manipulation without requiring detailed object knowledge or real-time feedback.","tags":["Robotics","Object Manipulation","Caging","Robust Control"],"title":"Caging in Time: A Framework for Robust Object Manipulation under Uncertainties and Limited Robot Perception","type":"publication"},{"authors":false,"categories":null,"content":"Our paper “Caging in Time: A Framework for Robust Object Manipulation under Uncertainties and Limited Robot Perception” has been accepted by International Journal of Robotics Research (IJRR).\n","date":1746057600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1746057600,"objectID":"ffb2ae752939ce5f4d32888ae72c370e","permalink":"https://robotpilab.github.io/post/2025_gaotian_ijrr/","publishdate":"2025-05-01T00:00:00Z","relpermalink":"/post/2025_gaotian_ijrr/","section":"post","summary":"Our paper “Caging in Time: A Framework for Robust Object Manipulation under Uncertainties and Limited Robot Perception” has been accepted by International Journal of Robotics Research (IJRR).","tags":null,"title":"Our paper \"Caging in Time, A Framework for Robust Object Manipulation under Uncertainties and Limited Robot Perception\" has been accepted by IJRR","type":"post"},{"authors":false,"categories":null,"content":"Our paper “Robust Peg-in-Hole Assembly under Uncertainties via Compliant and Interactive Contact-Rich Manipulation” has been accepted by Robotics: Science and Systems (RSS) 2025.\n","date":1744848000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1744848000,"objectID":"f8a28dffdd25483155c071a8a3dafb3b","permalink":"https://robotpilab.github.io/post/2025_yiting_rss/","publishdate":"2025-04-17T00:00:00Z","relpermalink":"/post/2025_yiting_rss/","section":"post","summary":"Our paper “Robust Peg-in-Hole Assembly under Uncertainties via Compliant and Interactive Contact-Rich Manipulation” has been accepted by Robotics: Science and Systems (RSS) 2025.","tags":null,"title":"Our paper \"Robust Peg-in-Hole Assembly under Uncertainties via Compliant and Interactive Contact-Rich Manipulation\" has been accepted by RSS 2025","type":"post"},{"authors":["Yiting Chen","Kenneth Kimble","Howard H. Qian","Podshara Chanrungmaneekul","Robert Seney","Kaiyu Hang"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1743465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1743465600,"objectID":"eeb7d6d6a6dac90530266c12cf83d368","permalink":"https://robotpilab.github.io/publication/peginhole/","publishdate":"2025-04-01T00:00:00Z","relpermalink":"/publication/peginhole/","section":"publication","summary":"We propose a robust, learning-free robotic peg-in-hole assembly system that leverages compliant, contact-rich interactions to eliminate both perception and execution uncertainties. By actively seeking contact and exploiting environmental constraints, our method enables reliable assembly across diverse scenarios without precise perception or prior training.","tags":["Robotics","Manipulation","Assembly","Uncertainty","Contact-rich interaction"],"title":"Robust Peg-in-Hole Assembly under Uncertainties via Compliant and Interactive Contact-Rich Manipulation","type":"publication"},{"authors":["Yu Sun","Berk Calli","Kenny Kimble","Francis wyffels","Victor-Louis De Gusseme","Kaiyu Hang","Salvatore D'Avella","Alessio Xompero","Andrea Cavallaro","Maximo A. Roa","Jose Avendano","Anastasia Mavrommati"],"categories":null,"content":"Published in IEEE Robotics \u0026amp; Automation Magazine (Volume: 31, Issue: 4, December 2024), Pages: 174-185\n","date":1733875200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733875200,"objectID":"ee04f5505a498bd1a09ab7ca9c92881d","permalink":"https://robotpilab.github.io/publication/rgmc/","publishdate":"2024-12-11T00:00:00Z","relpermalink":"/publication/rgmc/","section":"publication","summary":"The 2024 RGMC at ICRA featured five tracks testing autonomous robotic manipulation capabilities, drawing participation from teams worldwide to address challenges in manufacturing and essential manipulation skills.","tags":["Robotics","Competition","Manipulation","Grasping"],"title":"Robotic Grasping and Manipulation Competition at the 2024 IEEE/RAS International Conference on Robotics and Automation","type":"publication"},{"authors":false,"categories":null,"content":"Dr. Kaiyu Hang has been selected by the American Society of Mechanical Engineers (ASME) to receive the award of ASME Rising Star of Mechanical Engineering in recognition of his contributions to robotic manipulation.\nDr. Kaiyu Hang presented his work at the award ceremony in the 2024 International Mechanical Engineering Congress \u0026amp; Exposition (ASME IMECE), Portland, Oregon, USA.\n","date":1731974400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1731974400,"objectID":"f28cc75677b7e74904b39305edf70735","permalink":"https://robotpilab.github.io/post/hang_asme/","publishdate":"2024-11-19T00:00:00Z","relpermalink":"/post/hang_asme/","section":"post","summary":"Dr. Kaiyu Hang has been selected by the American Society of Mechanical Engineers (ASME) to receive the award of ASME Rising Star of Mechanical Engineering in recognition of his contributions to robotic manipulation.","tags":null,"title":"Dr. Kaiyu Hang has Been Awarded ASME Rising Star of Mechanical Engineering","type":"post"},{"authors":false,"categories":null,"content":"\rWe are excited to announce that our lab member, Hayden Webb, has placed 3rd place in the prestigious “Student Mechanism and Robotics Design Competition” at the 2024 ASME IDETC.\nHayden Webb at the award ceremony This is a very competitive competition where international teams from across the world apply to compete and showcase their engineering design projects. To apply, students submit a detailed report of their work showcasing its capabilities as well as highlighting their invention’s ingenuity, novelty, and robustness in engineering. For this reason, students who apply for this competition are expected to have produced some robot or mechanism that excels in its purpose and goes beyond standard engineering designs.\nFrom the initial applications, six finalists are selected and of these finalists, the top-ranked projects are awarded a travel grant to present their work in-person at the associated international ASME conference. Hayden was selected as a finalist and awarded a travel grant, exemplifying the work and exceptional level of engineering put into the project.\nHayden placed 3rd in the undergraduate category in the final round of competition after giving both a talk and presentation on the Roller Ring. This award demonstrates the high-quality research and engineering design that is developed in our lab and showcases Hayden’s extraordinary contribution to the field. We congratulate Hayden on this hard-fought achievement and look forward to more of what comes next in his work!\nVideo Presentation Watch Hayden’s presentation of the Roller Ring project: Watch the Roller Ring Presentation\n","date":1726358400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726358400,"objectID":"9c62f912ade95aa3f18f66cb5e5a79fa","permalink":"https://robotpilab.github.io/post/hayden_award/","publishdate":"2024-09-15T00:00:00Z","relpermalink":"/post/hayden_award/","section":"post","summary":"We are excited to announce that our lab member, Hayden Webb, has placed 3rd place in the prestigious “Student Mechanism and Robotics Design Competition” at the 2024 ASME IDETC.","tags":null,"title":"Hayden Webb won 3rd place in the \"Student Mechanism and Robotics Design Competition\" at the 2024 ASME IDETC","type":"post"},{"authors":["Joshua T. Grace","Podshara Chanrungmaneekul","Kaiyu Hang","Aaron M. Dollar"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1723408800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1723408800,"objectID":"2bc0321e34b3047b17e80da2e165fe83","permalink":"https://robotpilab.github.io/publication/josh/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/publication/josh/","section":"publication","summary":"Controlling robotic in-hand manipulation is complex due to the varying physics and required system knowledge. One model, the inverse Jacobian, translates desired object motions to hand controls, but acquiring it without sophisticated system models is challenging. Our method uses a particle filter-based scheme to self-identify inverse Jacobians, enabling underactuated hands to stably grasp during self-identification movements. This approach requires no prior knowledge and learns the system's inverse Jacobian through exploratory motions. Our system closely approximates the inverse Jacobian, performing manipulation tasks successfully. Experiments on a Yale Model O hand show sub-millimeter precision and real-time control up to 900Hz.","tags":["Source Themes"],"title":"Direct Self-Identification of Inverse Jacobians for Dexterous Manipulation Through Particle Filtering","type":"publication"},{"authors":["Podshara Chanrungmaneekul","Kejia Ren","Joshua T. Grace","Aaron M. Dollar","Kaiyu Hang"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1723408800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1723408800,"objectID":"589b76ff6b1fcda08e551e3788e2ba6e","permalink":"https://robotpilab.github.io/publication/interactive/","publishdate":"2014-01-01T00:00:00Z","relpermalink":"/publication/interactive/","section":"publication","summary":"We propose a self-calibration framework that allows robots to autonomously calibrate themselves into their workspaces using built-in force-torque sensors. Through compliant exploratory actions, the robot interactively explores the environment's geometries to estimate spatial relationships without relying on external sensors or human intervention. Experiments validate the effectiveness of our approach in accurately establishing robot-environment calibration.","tags":["Source Themes"],"title":"Interactive Robot-Environment Self-Calibration via Compliant Exploratory Actions","type":"publication"},{"authors":["Howard Qian","Yangxiao Lu","Kejia Ren","Gaotian Wang","Ninad Khargonkar","Yu Xiang","Kaiyu Hang"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1723408800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1723408800,"objectID":"125f193c7eec251c7a58f399389b8fb8","permalink":"https://robotpilab.github.io/publication/riseg/","publishdate":"2014-01-01T00:00:00Z","relpermalink":"/publication/riseg/","section":"publication","summary":"We introduce RISeg, a robot interactive perception framework that significantly improves unseen object segmentation in cluttered scenes. RISeg identifies regions of segmentation uncertainty from a static image-based model, introduces object motion through minimal robot interactions, and matches spatial twists of randomly sampled object frames to group them. This allows accumulated segmentation corrections without relying on object singulation. RISeg increases segmentation accuracy by 28.2% over static methods on real cluttered tabletop scenes.","tags":["Source Themes"],"title":"RISeg: Robot Interactive Object Segmentation via Body Frame-Invariant Features","type":"publication"},{"authors":["Gaotian Wang","Kejia Ren","Kaiyu Hang"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1723408800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1723408800,"objectID":"0f4c81763c7db7032d854980e2284ddd","permalink":"https://robotpilab.github.io/publication/uno/","publishdate":"2014-01-01T00:00:00Z","relpermalink":"/publication/uno/","section":"publication","summary":"We propose UNO Push, a unified framework for precise nonprehensile object pushing that jointly addresses system modeling, action generation, and control. Our approach approximates a system transition function via non-parametric learning using only a small number of exploratory actions, and integrates it with model predictive control. The approximated functions can be robustly transferred across novel objects and online updated for continuous improvement. Experiments on a real robot platform demonstrate that UNO Push is a light-weight and highly effective approach, achieving millimeter-level precision on novel objects without relying on a priori system information or large datasets.","tags":["Source Themes"],"title":"UNO Push: Unified Nonprehensile Object Pushing via Non-Parametric Estimation and Model Predictive Control","type":"publication"},{"authors":false,"categories":null,"content":"\rOur paper “Interactive Robot-Environment Self-Calibration via Compliant Exploratory Actions” has been accepted by IEEE/RSJ IEEE International Conference on Intelligent Robots and Systems (IEEE/RSJ IROS) 2024. Check out the video!\n","date":1720310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720310400,"objectID":"b9a8c84086451a4209164bb06bfb74ad","permalink":"https://robotpilab.github.io/post/self_cali_iros_24/","publishdate":"2024-07-07T00:00:00Z","relpermalink":"/post/self_cali_iros_24/","section":"post","summary":"Our paper “Interactive Robot-Environment Self-Calibration via Compliant Exploratory Actions” has been accepted by IEEE/RSJ IEEE International Conference on Intelligent Robots and Systems (IEEE/RSJ IROS) 2024. Check out the video!","tags":null,"title":"Our Paper \"Interactive Robot-Environment Self-Calibration via Compliant Exploratory Actions\" has been accepted by IEEE/RSJ IROS 2024","type":"post"},{"authors":false,"categories":null,"content":"\rOur paper “UNO Push: Unified Nonprehensile Object Pushing via Non-Parametric Estimation and Model Predictive Control” has been accepted by IEEE/RSJ IEEE International Conference on Intelligent Robots and Systems (IEEE/RSJ IROS) 2024. Check out the video!\n","date":1720310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720310400,"objectID":"eb3843a5726cf461e7bc1d42b4c2afcf","permalink":"https://robotpilab.github.io/post/uno_push_iros_24/","publishdate":"2024-07-07T00:00:00Z","relpermalink":"/post/uno_push_iros_24/","section":"post","summary":"Our paper “UNO Push: Unified Nonprehensile Object Pushing via Non-Parametric Estimation and Model Predictive Control” has been accepted by IEEE/RSJ IEEE International Conference on Intelligent Robots and Systems (IEEE/RSJ IROS) 2024.","tags":null,"title":"Our Paper \"UNO Push, Unified Nonprehensile Object Pushing via Non-Parametric Estimation and Model Predictive Control\" has been accepted by IEEE/RSJ IROS 2024","type":"post"},{"authors":false,"categories":null,"content":"\rWe are thrilled to announce that our esteemed lab member, Howard Qian, has been awarded the prestigious “Distinction in Research and Creative Works” by Rice University.\nThis is a university honor for graduating students that will be granted at Commencement and will appear on recipients’ transcripts and diplomas. To apply, CS students should submit a work or body of work that illustrates creativity, self-initiation, independence, perseverance, and dedication. In all cases, we expect that students who apply for distinction in undergraduate research in computer science will have produced something that goes beyond the traditional expectations of their coursework.\nFor Computer Science students, the application process involves submitting a work or body of work that goes beyond the traditional expectations of coursework. Howard’s successful application cited his contributions to the RISeg paper, with his first authorship and the paper’s acceptance to ICRA being key factors in receiving this honor.\nThis award underscores the high-quality research being conducted in our lab and highlights Howard’s exceptional contributions to the field. We congratulate Howard on this well-deserved recognition and look forward to his continued success in future endeavors.\n","date":1714003200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714003200,"objectID":"77feb0443914e80ab5b3fbd18bf306fc","permalink":"https://robotpilab.github.io/post/howard_award/","publishdate":"2024-04-25T00:00:00Z","relpermalink":"/post/howard_award/","section":"post","summary":"We are thrilled to announce that our esteemed lab member, Howard Qian, has been awarded the prestigious “Distinction in Research and Creative Works” by Rice University.\nThis is a university honor for graduating students that will be granted at Commencement and will appear on recipients’ transcripts and diplomas.","tags":null,"title":"Howard Qian has been Awarded \"Distinction in Research and Creative Works\" by Rice University","type":"post"},{"authors":false,"categories":null,"content":"\rOur paper “RISeg: Robot Interactive Object Segmentation via Body Frame-Invariant Features” has been accepted by IEEE-RAS International Conference on Robotics and Automation (IEEE-RAS ICRA). Check out the video!\nRISeg was also recently accepted to the “3DVRM workshop” at ICRA!\nRISeg is an interactive perception framework in which robot actions are selected based on a scene’s uncertainty heatmap representation and object segmentation is improved via a body frame-invariant feature.\nIn order to successfully perform manipulation tasks in new environments, such as grasping, robots must be proficient in segmenting unseen objects from the background and/or other objects. Previous works perform unseen object instance segmentation (UOIS) by training deep neural networks on large-scale data to learn RGB/RGB-D feature embeddings, where cluttered environments often result in inaccurate segmentations. We build upon these methods and introduce a novel approach to correct inaccurate segmentation, such as under-segmentation, of static image-based UOIS masks by using robot interaction and a designed body frame-invariant feature. We demonstrate that the relative linear and rotational velocities of frames randomly attached to rigid bodies due to robot interactions can be used to identify objects and accumulate corrected object-level segmentation masks. By introducing motion to regions of segmentation uncertainty, we are able to drastically improve segmentation accuracy in an uncertainty-driven manner with minimal, non-disruptive interactions (ca. 2-3 per scene). We demonstrate the effectiveness of our proposed interactive perception pipeline in accurately segmenting cluttered scenes by achieving an average object segmentation accuracy rate of 80.7%, an increase of 28.2% when compared with other state-of-the-art UOIS methods.\n","date":1709337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1709337600,"objectID":"6488d5a55e60ca041719c6a067409513","permalink":"https://robotpilab.github.io/post/riseg_icra_24/","publishdate":"2024-03-02T00:00:00Z","relpermalink":"/post/riseg_icra_24/","section":"post","summary":"Our paper “RISeg: Robot Interactive Object Segmentation via Body Frame-Invariant Features” has been accepted by IEEE-RAS International Conference on Robotics and Automation (IEEE-RAS ICRA). Check out the video!\nRISeg was also recently accepted to the “3DVRM workshop” at ICRA!","tags":null,"title":"Our Paper \"RISeg, Robot Interactive Object Segmentation via Body Frame-Invariant Features\" has been accepted by IEEE-RAS ICRA 2024, as well as its 3DVRM workshop","type":"post"},{"authors":false,"categories":null,"content":"\rWe are organizing the 9th Robotic Grasping and Manipulation Competition (RGMC) at IEEE-RAS ICRA 2024 in Yokohama, Japan, on May 13-17. Dr. Hang is the lead organizer of the In-Hand Manipulation Competition (Sub-Track 2) within the Essential Skill Track. We look forward to seeing everyone in the competition!\n","date":1707433980,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707433980,"objectID":"d233b12f2edeed3c380337fdacddd6dc","permalink":"https://robotpilab.github.io/post/rgmc/","publishdate":"2024-02-08T23:13:00Z","relpermalink":"/post/rgmc/","section":"post","summary":"We are organizing the 9th Robotic Grasping and Manipulation Competition (RGMC) at IEEE-RAS ICRA 2024 in Yokohama, Japan, on May 13-17. Dr. Hang is the lead organizer of the In-Hand Manipulation Competition (Sub-Track 2) within the Essential Skill Track.","tags":null,"title":"In-Hand Manipulation Competition in the 9th RGMC","type":"post"},{"authors":["Podshara Chanrungmaneekul","Kejia Ren","Joshua T. Grace","Aaron M. Dollar","Kaiyu Hang"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1690566921,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690566921,"objectID":"a91747ca0f7f515dab67c68e548776ab","permalink":"https://robotpilab.github.io/publication/pod/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/pod/","section":"publication","summary":"Building hand-object models for dexterous in-hand manipulation remains a crucial and open problem. Major challenges include the difficulty of obtaining the geometric and dynamic models of the hand, object, and time-varying contacts, as well as the inevitable physical and perceptual uncertainties. Instead of building accurate models to map between the actuation inputs and the object motions, this work proposes enabling the hand-object systems to continuously approximate their local models via a self-identification process where an underlying manipulation model is estimated through a small number of exploratory actions and non-parametric learning. With a very small number of data points, as opposed to most data-driven methods, our system self-identifies the underlying manipulation models online through exploratory actions and non-parametric learning. By integrating the self-identified hand-object model into a model predictive control framework, the proposed system closes the control loop to provide high accuracy in-hand manipulation. Furthermore, the proposed self-identification can adaptively trigger online updates through additional exploratory actions as soon as the self-identified local models render large discrepancies against the observed manipulation outcomes. We implemented the proposed approach on a sensorless underactuated Yale Model O hand with a single external camera to observe the object's motion. With extensive experiments, we show that the proposed self-identification approach can enable accurate and robust dexterous manipulation without requiring an accurate system model or a large amount of data for offline training.","tags":[],"title":"Non-Parametric Self-Identification and Model Predictive Control of Dexterous In-Hand Manipulation","type":"publication"},{"authors":false,"categories":null,"content":"Kaiyu Hang has won a National Science Foundation CAREER Award to develop robots that can physically interact with the world through compliance- and motion-based manipulation funnels.\n","date":1688076494,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688076494,"objectID":"5685b3fd1440f7c5b1a66695ff9ecb1b","permalink":"https://robotpilab.github.io/post/national-science-foundation-career-award/","publishdate":"2023-06-29T22:08:14.077Z","relpermalink":"/post/national-science-foundation-career-award/","section":"post","summary":"Kaiyu Hang has won a National Science Foundation CAREER Award to develop robots that can physically interact with the world through compliance- and motion-based manipulation funnels.","tags":null,"title":"National Science Foundation CAREER Award","type":"post"},{"authors":["Kejia Ren","Podshara Chanrungmaneekul","Lydia E. Kavraki","Kaiyu Hang"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1687974921,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687974921,"objectID":"279fcc847959aa7198c1989aea0a6c51","permalink":"https://robotpilab.github.io/publication/kinodynamic-rapidly-exploring-random-forest-for-rearrangement-based-nonprehensile-manipulation/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/kinodynamic-rapidly-exploring-random-forest-for-rearrangement-based-nonprehensile-manipulation/","section":"publication","summary":"Our approach combines local rearrangement and global action optimization for nonprehensile manipulation, with free-space transit motions between constrained rearranging actions. We use a kinodynamic planning framework to search in multiple regions, allowing for global exploration of relevant subspaces and effective switches between local actions. The framework can adapt to real-world uncertainties and improves efficiency and effectiveness while remaining robust against uncertainties, as shown by extensive experiments.","tags":[],"title":"Kinodynamic Rapidly-exploring Random Forest for Rearrangement-Based Nonprehensile Manipulation","type":"publication"},{"authors":["Yangxiao Lu","Ninad Khargonkar","Zesheng Xu","Charles Averill","Kamalesh Palanisamy","Kaiyu Hang","Yunhui Guo","Nicholas Ruozzi","Yu Xiang"],"categories":null,"content":"","date":1687971774,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687971774,"objectID":"25d1df9fe0c5bc6005b3caa9b85efc24","permalink":"https://robotpilab.github.io/publication/self-supervised-unseen-object-instance-segmentation-via-long-term-robot-interaction/","publishdate":"2023-06-28T17:02:54.9Z","relpermalink":"/publication/self-supervised-unseen-object-instance-segmentation-via-long-term-robot-interaction/","section":"publication","summary":"Our robot system improves segmentation of unseen object instances in the real world using multi-object tracking and video object segmentation on images collected via robot pushing. This self-supervised approach generates segmentation masks of all objects in the images, including those where objects are close together and segmentation errors are common. We fine-tune segmentation networks trained on synthetic data with real-world data collected by our system, significantly improving accuracy across domains and enhancing robotic grasping of previously unseen objects.","tags":null,"title":"Self-Supervised Unseen Object Instance Segmentation via Long-Term Robot Interaction","type":"publication"},{"authors":false,"categories":null,"content":"Rice University Computer Scientists presented research on enabling robots to rearrange objects without grasping them at the 2023 International Conference on Robotics and Automation.\n","date":1687471694,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687471694,"objectID":"4b5585741c184c05bc4be1df724ee50e","permalink":"https://robotpilab.github.io/post/kejia-ren/","publishdate":"2023-06-22T22:08:14.077Z","relpermalink":"/post/kejia-ren/","section":"post","summary":"Rice University Computer Scientists presented research on enabling robots to rearrange objects without grasping them at the 2023 International Conference on Robotics and Automation.","tags":null,"title":"Rice CS team helps robots rearrange objects without gripping them","type":"post"},{"authors":false,"categories":null,"content":"\rOur paper “Non-Parametric Self-Identification and Model Predictive Control of Dexterous In-Hand Manipulation” has been accepted by IEEE/RSJ IEEE International Conference on Intelligent Robots and Systems (IEEE/RSJ IROS). Check out the video!\n","date":1687385407,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687385407,"objectID":"1d0015c30f3f5aa97687715ac90a43e6","permalink":"https://robotpilab.github.io/post/paper-non-parametric-self-identification/","publishdate":"2023-06-21T22:10:07.833Z","relpermalink":"/post/paper-non-parametric-self-identification/","section":"post","summary":"Our paper “Non-Parametric Self-Identification and Model Predictive Control of Dexterous In-Hand Manipulation” has been accepted by IEEE/RSJ IEEE International Conference on Intelligent Robots and Systems (IEEE/RSJ IROS). Check out the video!","tags":null,"title":"Our Paper \"Non-Parametric Self-Identification and Model Predictive Control of Dexterous In-Hand Manipulation\" has been accepted by IEEE/RSJ IROS","type":"post"},{"authors":[],"categories":null,"content":"\rRobust, dexterous manipulation in unstructured environments remains a research problem for those working in both academia and industry. Encapsulating a plethora of potential use cases—from logistics-based packing problems to human-robot service settings—the interactions between a robot and its environment are often difficult to plan and execute precisely, as there will always be some degree of uncertainty in the model of the robot or its environment. This uncertainty has historically elicited conflict within the robot’s internal control schema, as it required both positions and forces of the actuators to be balanced appropriately as to satisfy task requirements. System compliance—either in a software-based or a hardware-based solution—has been largely the key to enabling a robot to overcome such environmental or system-modeling uncertainties. This inherent adaptability in the robot’s kinematic structure can generally simplify planning and control of the robot, which has in turn enabled fundamental advancements in robot manipulation. In this workshop, we will explore the state-of-the in compliance-enabled robot manipulation on numerous fronts. Panelists and speakers will discuss how compliance, and other similar paradigms, have changed formulations and processes for planning, control, design, sensing, learning, optimization, etc. for such robot systems. ","date":1683723600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683723600,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://robotpilab.github.io/event/example/","publishdate":"2023-05-19T18:50:24.126Z","relpermalink":"/event/example/","section":"event","summary":"ICRA workshop","tags":[],"title":"\"Compliant Robot Manipulation: Challenges and New Opportunities\" Workshop","type":"event"},{"authors":null,"categories":null,"content":"\rThe Texas Regional Robotics Symposium (TEROS) aims to bring together Texas robotics researchers. TEROS is a one-day event with research talks, poster presentations, and social activities. The goal of TEROS is to share exciting ideas and build new connections in the robotics research community in Texas.\nAfter the success of its first edition at the University of Texas at Austin, the second edition of TEROS is coming to Houston. We are excited to announce that TEROS 2023 will be held on April 14, 2023 at Rice University’s BioScience Research Collaborative (BRC). We are proud to share that, similar to its first edition, this event is free for attendees. The event will be held in-person only and the tickets are limited, so please register at the earliest. We look forward to seeing you soon in Houston.\n","date":1681473642,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1681473642,"objectID":"29b39ede78dd76ea5a1802aa46d82f40","permalink":"https://robotpilab.github.io/event/teros/","publishdate":"2023-05-19T18:43:42.08Z","relpermalink":"/event/teros/","section":"event","summary":"We are excited to see you in-person this Friday, April 14th, for the second Texas Regional Robotics Symposium (TEROS). The symposium will feature an exciting program, with \n\nKeynotes from Professors Ann Majewicz Fey, Dezhen Song, Moshe Vardi, and Luis Sentis;\n8 spotlight talks from distinguished speakers, spanning the different sub-areas of robotics; and\n4 industry booths and 30 poster presentations, from 10 institutions across Texas.","tags":null,"title":"TEROS","type":"event"},{"authors":false,"categories":null,"content":"Our paper “Self-Supervised Unseen Object Instance Segmentation via Long-Term Robot Interaction” has been accepted by Robotics: Science and Systems (RSS 2023). Check out the video!\n","date":1677736800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677736800,"objectID":"be2bd15f022f0d83fe9ffd743881e70c","permalink":"https://robotpilab.github.io/post/20-12-01-wowchemy-prize/","publishdate":"2023-03-02T06:00:00Z","relpermalink":"/post/20-12-01-wowchemy-prize/","section":"post","summary":"Our paper “Self-Supervised Unseen Object Instance Segmentation via Long-Term Robot Interaction” has been accepted by Robotics: Science and Systems (RSS 2023). Check out the video!","tags":null,"title":"Our Paper \"Self-Supervised Unseen Object Instance Segmentation via Long-Term Robot Interaction\" has been accepted by RSS 2023","type":"post"},{"authors":false,"categories":null,"content":"\rIn IEEE-RAS ICRA 2023, we will organize the second workshop on “Compliant Robot Manipulation: Challenges and New Opportunities”. Distinguished speakers from both academia and industry will join to share their brilliant research progress and throughts. Poster contributions with oral presentation opportunities will be an important part of the workshop. Please read more details here .\n","date":1671750692,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671750692,"objectID":"671d495b8e784f2fbaaa7aed81de62af","permalink":"https://robotpilab.github.io/post/workshop-on-compliant-robot-manipulation-challenges-and-new-opportunities-in-ieee-ras-icra-2023/","publishdate":"2022-12-22T23:11:32.678Z","relpermalink":"/post/workshop-on-compliant-robot-manipulation-challenges-and-new-opportunities-in-ieee-ras-icra-2023/","section":"post","summary":"In IEEE-RAS ICRA 2023, we will organize the second workshop on “Compliant Robot Manipulation: Challenges and New Opportunities”. Distinguished speakers from both academia and industry will join to share their brilliant research progress and throughts.","tags":null,"title":"Workshop on \"Compliant Robot Manipulation: Challenges and New Opportunities\" in IEEE-RAS ICRA 2023","type":"post"},{"authors":false,"categories":null,"content":"\rOur paper “Kinodynamic Rapidly-exploring Random Forest for Rearrangement-Based Nonprehensile Manipulation” has been accepted by IEEE-RAS International Conference on Robotics and Automation (IEEE-RAS ICRA). Check out the video!\n","date":1669939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669939200,"objectID":"2a0ec8a990dbd78a00c4e15a09364b00","permalink":"https://robotpilab.github.io/post/20-12-02-icml-best-paper/","publishdate":"2022-12-02T00:00:00Z","relpermalink":"/post/20-12-02-icml-best-paper/","section":"post","summary":"Our paper “Kinodynamic Rapidly-exploring Random Forest for Rearrangement-Based Nonprehensile Manipulation” has been accepted by IEEE-RAS International Conference on Robotics and Automation (IEEE-RAS ICRA). Check out the video!","tags":null,"title":"Our Paper \"Kinodynamic Rapidly-exploring Random Forest for Rearrangement-Based Nonprehensile Manipulation\" has been accepted by IEEE-RAS ICRA 2023","type":"post"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://robotpilab.github.io/contact/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"Contact","type":"landing"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"8795eba9bd19b87b0616d17da3c16590","permalink":"https://robotpilab.github.io/join/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/join/","section":"","summary":"","tags":null,"title":"Join","type":"landing"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://robotpilab.github.io/people/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"People","type":"landing"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"b0d61e5cbb7472bf320bf0ef2aaeb977","permalink":"https://robotpilab.github.io/tour/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/tour/","section":"","summary":"","tags":null,"title":"Tour","type":"landing"},{"authors":["Howard Qian"],"categories":null,"content":"\rImproving the robot capability of object visual understanding by shifting the traditional robot perception paradigm from “wait-to-observe” to “seek-to-observe” through closing the loop between uncertainty-aware perception and robot exploratory manipulation.\n","date":1666483200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666483200,"objectID":"a47970aec351c7fde22f174c46567a28","permalink":"https://robotpilab.github.io/projects/active/","publishdate":"2022-10-23T00:00:00Z","relpermalink":"/projects/active/","section":"projects","summary":"Improving the robot capability of object visual understanding by shifting the traditional robot perception paradigm from “wait-to-observe” to “seek-to-observe” through closing the loop between uncertainty-aware perception and robot exploratory manipulation.","tags":[],"title":"Interactive Object Perception","type":"projects"},{"authors":["Kejia Ren","Lydia E. Kavraki","Kaiyu Hang"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656633600,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://robotpilab.github.io/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"This paper presents a planning framework for robot manipulation in cluttered environments that dynamically controls the planning horizon and interleaves planning and execution. The framework allows for flexible interaction with all objects and improves planning efficiency, robustness against physical uncertainties, and task success rate under limited time budgets.","tags":[],"title":"Rearrangement-Based Manipulation via Kinodynamic Planning and Dynamic Planning Horizons","type":"publication"},{"authors":["Andrew S. Morgan","Kaiyu Hang","Bowen Wen","Kostas Bekris","and Aaron M. Dollar"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1646355600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646355600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://robotpilab.github.io/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"This work presents a method for achieving complete SO(3) finger gating control of grasped objects against gravity, using a manipulation planner that operates via orthogonal safe modes of a compliant, underactuated hand absent of tactile sensors or joint encoders. The method takes advantage of system compliance to allow the hand to more easily switch contacts while maintaining a stable grasp, and uses a low-latency 6D pose object tracker for online feedback and adaptive recovery from trajectory deviations. The method is demonstrated on a real robot manipulating both convex and non-convex objects, and represents a valuable step towards realizing true robot in-hand manipulation capabilities.","tags":["Source Themes"],"title":"Complex In-Hand Manipulation via Compliance-Enabled Finger Gaiting and Multi-Modal Planning","type":"publication"},{"authors":false,"categories":null,"content":"\rOur work on Robot Self-Identification has been selected by Science Robotics as the Top 5 Editor’s Picks of 2021! Read Here .\n","date":1639782780,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639782780,"objectID":"134142f582f3bfecba90d594ccd74f1a","permalink":"https://robotpilab.github.io/post/robot-self-identification-selected-by-science-robotics-as-the-top-5-editors-picks-of-2021/","publishdate":"2021-12-17T23:13:00Z","relpermalink":"/post/robot-self-identification-selected-by-science-robotics-as-the-top-5-editors-picks-of-2021/","section":"post","summary":"Our work on Robot Self-Identification has been selected by Science Robotics as the Top 5 Editor’s Picks of 2021! Read Here .","tags":null,"title":"Robot Self-Identification has been selected by Science Robotics as the Top 5 Editor's Picks of 2021","type":"post"},{"authors":["Hayden Webb"],"categories":null,"content":"\rDexterous in-hand manipulation without lifting a finger using a class of novel compliant robot hands featured by active surfaces.\n","date":1635120000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635120000,"objectID":"a3e5c5af2db9b9a3d336768d639b8ca0","permalink":"https://robotpilab.github.io/projects/hand/","publishdate":"2021-10-25T00:00:00Z","relpermalink":"/projects/hand/","section":"projects","summary":"Dexterous in-hand manipulation without lifting a finger using a class of novel compliant robot hands featured by active surfaces.","tags":[],"title":"Active Surface-based Compliant Robot Hands","type":"projects"},{"authors":["Kejia Ren","Gaotian Wang"],"categories":null,"content":"\rEnabling multi-modal robot nonprehensile manipulation by concurrently building multiple motion trees and adaptively switching between them.\n","date":1635033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635033600,"objectID":"93d4266e478e74e97a8fbdfa8991b4bd","permalink":"https://robotpilab.github.io/projects/example/","publishdate":"2021-10-24T00:00:00Z","relpermalink":"/projects/example/","section":"projects","summary":"Enabling multi-modal robot nonprehensile manipulation by concurrently building multiple motion trees and adaptively switching between them.","tags":[],"title":"Nonprehensile Manipulation through Forest-based Planning with Dynamic Planning Horizons","type":"projects"},{"authors":["Kejia Ren","Gaotian Wang"],"categories":null,"content":"\rWe investigate “manipulation funnels”, which effectively converge complex manipulation states towards strictly smaller subsets, to provide robust manipulation against various uncertainties, as well as creating new manipulation possibilities that were traditionally infeasible.\n","date":1634342400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634342400,"objectID":"80eb1c106eede2069d77a71aee63ff5a","permalink":"https://robotpilab.github.io/projects/funnel/","publishdate":"2021-10-16T00:00:00Z","relpermalink":"/projects/funnel/","section":"projects","summary":"We investigate “manipulation funnels”, which effectively converge complex manipulation states towards strictly smaller subsets, to provide robust manipulation against various uncertainties, as well as creating new manipulation possibilities that were traditionally infeasible.","tags":[],"title":"Exploring Robust Robot Manipulation through Compliance-Based and Motion-based Manipulation Funnels","type":"projects"},{"authors":["Kejia Ren","Podshara Chanrungmaneekul"],"categories":null,"content":"\rCombining modern state estimation techniques with specialized adaptable robot hardware to enable online discovery and updating of manipulation “controllers” that require minimal sensing and are highly robust to uncertainty.\n","date":1634169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634169600,"objectID":"35dadcc9e3dd7d2472b7d9ab0efedfe8","permalink":"https://robotpilab.github.io/projects/teros/","publishdate":"2021-10-14T00:00:00Z","relpermalink":"/projects/teros/","section":"projects","summary":"Combining modern state estimation techniques with specialized adaptable robot hardware to enable online discovery and updating of manipulation “controllers” that require minimal sensing and are highly robust to uncertainty.","tags":[],"title":"Self-Identification for Robot Manipulation under Uncertainty Aided by Passive Adaptability","type":"projects"},{"authors":["Haoran Song","Anastasiia Varava","Oleksandr Kravchenko","Danica Kragic","Michael Y. Wang","Florian T. Pokorny","Kaiyu Hang"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1628713860,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628713860,"objectID":"5b1ce9a4b4eb394f9d7433b1065c8ded","permalink":"https://robotpilab.github.io/publication/herding-by-caging-a-formation-based-motion-planning-framework-for-guiding-mobile-agents/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/herding-by-caging-a-formation-based-motion-planning-framework-for-guiding-mobile-agents/","section":"publication","summary":"We propose a solution to the problem of herding by caging: given a set of mobile robots (called herders) and a group of moving agents (called sheep), we guide the sheep to a target location without letting them escape from the herders along the way. We model the interaction between the herders and the sheep by defining virtual “repulsive forces” pushing the sheep away from the herders. This enables the herders to partially control the motion of the sheep. We formalize this behavior topologically by applying the notion of caging, a concept used in robotic manipulation. We demonstrate that our approach is provably correct in the sense that the sheep cannot escape from the robots under our assumed motion model. We propose an RRT-based path planning algorithm for herding by caging, demonstrate its probabilistic completeness, and evaluate it in simulations as well as on a group of real mobile robots.","tags":["Source Themes"],"title":"Herding by Caging: A Formation-Based Motion Planning Framework for Guiding Mobile Agents","type":"publication"},{"authors":["Kaiyu Hang","Walter G. Bircher","Andrew S. Morgan","Aaron M. Dollar"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1620759060,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620759060,"objectID":"e19b22bfbb7ec57a1ed7dbeac6c01a41","permalink":"https://robotpilab.github.io/publication/manipulation-for-self-identification-and-self-identification-for-better-manipulation/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/manipulation-for-self-identification-and-self-identification-for-better-manipulation/","section":"publication","summary":"The process of modeling a series of hand-object parameters is crucial for precise and controllable robotic in-hand manipulation because it enables the mapping from the hand’s actuation input to the object’s motion to be ob-tained. Without assuming that most of these model parameters are known a priori or can be easily estimated by sensors, we focus on equipping robots with the ability to actively self-identify necessary model parameters using minimal sensing. Here, we derive algorithms, on the basis of the concept of virtual linkage-based representations (VLRs), to self-identify the underlying mechanics of hand-object systems via exploratory manipulation actions and probabilistic reasoning and, in turn, show that the self-identified VLR can enable the control of precise in-hand ma-nipulation. To validate our framework, we instantiated the proposed system on a Yale Model O hand without joint encoders  or  tactile  sensors.  The  passive  adaptability  of  the  underactuated  hand  greatly  facilitates  the  self-identification  process, because they naturally secure stable hand-object interactions during random exploration. Relying solely on an in-hand camera, our system can effectively self-identify the VLRs, even when some fingers are replaced with novel designs. In addition, we show in-hand manipulation applications of handwriting, marble maze playing, and cup stacking to demonstrate the effectiveness of the VLR in precise in-hand manipulation control.","tags":["Source Themes"],"title":"Manipulation for self-Identification, and self-Identification for better manipulation","type":"publication"},{"authors":["Silvia Cruciani","Balakumar Sundaralingam","Kaiyu Hang","Vikash Kumar","Tucker Hermans","Danica Kragic"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1597178400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597178400,"objectID":"de86a9c0b88e12ce823a227ed064c557","permalink":"https://robotpilab.github.io/publication/benchmarking-in-hand-manipulation/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/benchmarking-in-hand-manipulation/","section":"publication","summary":"The purpose of this benchmark is to evaluate the planning and control aspects of robotic in-hand manipulation systems. The goal is to assess the system's ability to change the position of a handheld object using the fingers, environment, or a combination of both. We provide examples of initial and goal states (i.e., static object poses and fingertip locations) for various in-hand manipulation tasks, given an object surface mesh from the YCB dataset. We further propose metrics that measure the error in reaching the goal state from a specific initial state. When aggregated across all tasks, these metrics also serve as a measure of the system's in-hand manipulation capability. We provide supporting software, task examples, and evaluation results associated with the benchmark.","tags":["Source Themes"],"title":"Benchmarking In-Hand Manipulation","type":"publication"},{"authors":["Andrew S. Morgan","Kaiyu Hang","Walter G. Bircher","Fadi M. Alladkani","Abhinav Gandhi","Berk Calli","Aaron M. Dollar"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1597178280,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597178280,"objectID":"fd5c8f62c961780ca5ed5ccc3529dc80","permalink":"https://robotpilab.github.io/publication/benchmarking-cluttered-robot-pick-and-place-manipulation-with-the-box-and-blocks-test/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/benchmarking-cluttered-robot-pick-and-place-manipulation-with-the-box-and-blocks-test/","section":"publication","summary":"In this work, we propose a pick-and-place benchmark to assess the manipulation capabilities of a robotic system. The benchmark is based on the Box and Blocks Test (BBT), a task that has been utilized for decades by the rehabilitation community to assess unilateral gross manual dexterity in humans. We propose three robot benchmarking protocols in this work that hold true to the spirit of the original clinical tests: the Modified-BBT, the Targeted-BBT, and the Standard-BBT. These protocols can be implemented by the greater robotics research community, as the physical BBT setup has been widely distributed with the Yale-CMU-Berkeley (YCB) Object and Model Set. The difficulty of the three protocols increases sequentially, adding a new performance component at each level, and therefore aiming to assess various aspects of the system separately. Clinical task-time norms are summarized for able-bodied human participants. We provide baselines for all three protocols with off-the-shelf planning and perception algorithms on a Barrett WAM and a Franka Emika Panda manipulator and compare the results with human performance.","tags":["Source Themes"],"title":"Benchmarking Cluttered Robot Pick-and-Place Manipulation with the Box and Blocks Test","type":"publication"},{"authors":["Yu Zheng","Kaiyu Hang"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1597178160,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597178160,"objectID":"0592ef655061d566c4cacc81c183831f","permalink":"https://robotpilab.github.io/publication/calculating-the-support-function-of-complex-continuous-surfaces-with-applications-to-minimum-distance-computation-and-optimal-grasp-planning/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/calculating-the-support-function-of-complex-continuous-surfaces-with-applications-to-minimum-distance-computation-and-optimal-grasp-planning/","section":"publication","summary":"\"The support function of a surface\" is a fundamental concept in mathematics and a crucial operation for algorithms in robotics, such as those for collision detection and grasp planning. It is possible to calculate the support function of a convex body in closed form. However, for complex continuous surfaces, especially non-convex ones, this calculation can be far more difficult, and no general solution is available so far. This limits the applicability of related algorithms. This paper presents a branch-and-bound (B\u0026B) algorithm to calculate the support function of complex continuous surfaces. An upper bound of the support function over a surface domain is derived. While a surface domain is divided into subdomains, the upper bound of the support function over any subdomain is proved to be not greater than the one over the original domain. Then, as the B\u0026B algorithm sequentially divides the surface domain by dividing its subdomain having a greater upper bound than the others, the maximum upper bound over all subdomains is monotonically decreasing and converges to the exact value of the desired support function. Furthermore, with the aid of the B\u0026B algorithm, this paper derives new algorithms for the minimum distance between complex continuous surfaces and for globally optimal grasps on objects with continuous surfaces. A number of numerical examples are provided to demonstrate the effectiveness of the proposed algorithms.","tags":["Source Themes"],"title":"Calculating the Support Function of Complex Continuous Surfaces With Applications to Minimum Distance Computation and Optimal Grasp Planning","type":"publication"},{"authors":["Andrew S. Morgan","Kaiyu Hang","Aaron M. Dollar"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1597178076,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597178076,"objectID":"fe8d6bab8d9d1d0fe9d2cb5ef3fa7fab","permalink":"https://robotpilab.github.io/publication/object-agnostic-dexterous-manipulation-of-partially-constrained-trajectories/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/object-agnostic-dexterous-manipulation-of-partially-constrained-trajectories/","section":"publication","summary":"We address the problem of controlling a partially constrained trajectory of the manipulation frame - an arbitrary frame of reference rigidly attached to the object - as the desired motion about this frame is often underdefined. This may be apparent, for example, when the task requires control only about the translational dimensions of the manipulation frame, with disregard to the rotational dimensions. This scenario complicates the computation of the grasp frame trajectory, as the mobility of the mechanism is likely limited due to the constraints imposed by the closed kinematic chain. In this letter, we address this problem by combining a learned, object-agnostic manipulation model of the gripper with Model Predictive Control (MPC). This combination facilitates an approach to simple vision-based control of robotic hands with generalized models, enabling a single manipulation model to extend to different task requirements. By tracking the hand-object configuration through vision, the proposed framework is able to accurately control the trajectory of the manipulation frame along translational, rotational, or mixed trajectories. We provide experiments quantifying the utility of this framework, analyzing its ability to control different objects over varied horizon lengths and optimization iterations, and finally, we implement the controller on a physical system.","tags":["Source Themes"],"title":"Object-Agnostic Dexterous Manipulation of Partially Constrained Trajectories","type":"publication"},{"authors":["Haoran Song","Joshua A. Haustein","Weihao Yuan","Kaiyu Hang","Michael Y. Wang","Danica Kragic","Johannes A. Stork"],"categories":null,"content":"","date":1593379920,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593379920,"objectID":"94e764df3fc2791e6bea9df58fbdc44b","permalink":"https://robotpilab.github.io/publication/multi-object-rearrangement-with-monte-carlo-tree-search-a-case-study-on-planar-nonprehensile-sorting/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/multi-object-rearrangement-with-monte-carlo-tree-search-a-case-study-on-planar-nonprehensile-sorting/","section":"publication","summary":"The article proposes using Monte Carlo tree search with a task-specific heuristic function to solve a planar non-prehensile sorting task where a robot needs to push densely packed objects into a configuration where classes are separated. The algorithm is evaluated on various simulated and real-world sorting tasks and is capable of reliably sorting large numbers of objects, including convex and non-convex objects and convex objects in the presence of immovable obstacles.","tags":[],"title":"Multi-Object Rearrangement with Monte Carlo Tree Search: A Case Study on Planar Nonprehensile Sorting","type":"publication"},{"authors":["Weihao Yuan","Kaiyu Hang","Haoran Song","Danica Kragic","Michael Y. Wang","Johannes A. Stork"],"categories":null,"content":"","date":1565644620,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565644620,"objectID":"8de69aa4b09946f6fa632d3e168be20e","permalink":"https://robotpilab.github.io/publication/reinforcement-learning-in-topology-based-representation-for-human-body-movement-with-whole-arm-manipulation/","publishdate":"2019-08-12T21:17:00Z","relpermalink":"/publication/reinforcement-learning-in-topology-based-representation-for-human-body-movement-with-whole-arm-manipulation/","section":"publication","summary":"Moving a human body or a large and bulky object may require the strength of whole-arm manipulation (WAM). This type of manipulation places the load on the robot's arms and relies on global properties of the interaction to succeed—rather than local contacts such as grasping or non-prehensile pushing. In this paper, we learn to generate motions that enable WAM for holding and transporting humans in certain rescue or patient care scenarios. We model the task as a reinforcement learning problem to provide a robot behavior that can directly respond to external perturbations and human motion. For this, we represent global properties of the robot-human interaction with topology-based coordinates that are computed from arm and torso positions. These coordinates also allow transferring the learned policy to other body shapes and sizes. For training and evaluation, we simulate a dynamic sea rescue scenario and show in quantitative experiments that the policy can solve unseen scenarios with differently shaped humans, floating humans, or with perception noise. Our qualitative experiments show the subsequent transporting after holding is achieved, and we demonstrate that the policy can be directly transferred to a real-world setting.","tags":null,"title":"Reinforcement Learning in Topology-based Representation for Human Body Movement with Whole Arm Manipulation","type":"publication"},{"authors":["Weihao Yuan","Kaiyu Hang","Danica Kragic","Michael Y. Wang","Johannes A. Stork"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1565556720,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565556720,"objectID":"0550037078dfdbd4356f00050e66adb8","permalink":"https://robotpilab.github.io/publication/end-to-end-nonprehensile-rearrangement-with-deep-reinforcement-learning-and-simulation-to-reality-transfer/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/end-to-end-nonprehensile-rearrangement-with-deep-reinforcement-learning-and-simulation-to-reality-transfer/","section":"publication","summary":"Nonprehensile rearrangement is the problem of controlling a robot to interact with objects through pushing actions in order to reconfigure the objects into a predefined goal pose. In this work, we rearrange one object at a time in an environment with obstacles using an end-to-end policy that maps raw pixels as visual input to control actions without any form of engineered feature extraction. To reduce the amount of training data that needs to be collected using a real robot, we propose a simulation-to-reality transfer approach. In the first step, we model the nonprehensile rearrangement task in simulation and use deep reinforcement learning to learn a suitable rearrangement policy, which requires hundreds of thousands of example actions for training. Thereafter, we collect a small dataset of only 70 episodes of real-world actions as supervised examples for adapting the learned rearrangement policy to real-world input data. In this process, we make use of newly proposed strategies for improving the reinforcement learning process, such as heuristic exploration and the curation of a balanced set of experiences. We evaluate our method in both simulation and real settings using a Baxter robot to show that the proposed approach can effectively improve the training process in simulation, as well as efficiently adapt the learned policy to the real-world application, even when the camera pose is different from simulation. Additionally, we show that the learned system can not only provide adaptive behavior to handle unforeseen events during executions, such as distracting objects, sudden changes in the positions of the objects, and obstacles, but also can deal with obstacle shapes that were not present in the training process.","tags":["Source Themes"],"title":"End-to-End Nonprehensile Rearrangement with Deep Reinforcement Learning and Simulation-to-Reality Transfer","type":"publication"},{"authors":["Kaiyu Hang","Andrew S. Morgan","Aaron M. Dollar"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1565556600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565556600,"objectID":"65b21edf263a68b75f97b6f8aa6e1859","permalink":"https://robotpilab.github.io/publication/pre-grasp-sliding/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/pre-grasp-sliding/","section":"publication","summary":"We address the problem of pre-grasp sliding manipulation, which is an essential skill when a thin object cannot be directly grasped from a flat surface. Leveraging the passive reconfigurability of soft, compliant, or underactuated robotic hands, we formulate this problem as an integrated motion and grasp planning problem and plan the manipulation directly in the robot configuration space. Instead of explicitly precomputing a pair of valid start and goal configurations and then planning a path to connect them in a separate step, our planner actively samples start and goal robot configurations from configuration sampleable regions modeled from the geometries of the object and support surface. While randomly connecting the sampled start and goal configurations in pairs, the planner verifies whether any connected pair can achieve the task to finally confirm a solution. The proposed planner is implemented and evaluated both in simulation and on a real robot. Given the inherent compliance of the employed Yale T42 hand, we relax the motion constraints and show that the planning performance is significantly boosted. Moreover, we show that our planner outperforms two baseline planners and that it can deal with objects and support surfaces of arbitrary geometries and sizes.","tags":["Source Themes"],"title":"Pre-Grasp Sliding Manipulation of Thin Objects Using Soft, Compliant, or Underactuated Hands","type":"publication"},{"authors":["Kaiyu Hang","Walter G. Bircher","Andrew S. Morgan","and Aaron M. Dollar"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1565556300,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565556300,"objectID":"0abc358f967626fe03c7ec11018f1271","permalink":"https://robotpilab.github.io/publication/hand-object-configuration-estimation-using-particle-filters-for-dexterous-in-hand-manipulation/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/hand-object-configuration-estimation-using-particle-filters-for-dexterous-in-hand-manipulation/","section":"publication","summary":"We consider the problem of dexterous manipulation with a focus on unknown or uncertain hand-object parameters, such as hand configuration, object pose within the hand, and contact positions. In this work, we formulate a generic framework for hand-object configuration estimation using underactuated hands as an example. Due to the passive reconfigurability and lack of encoders in the hand's joints, it is challenging to estimate, plan, and actively control underactuated manipulation. By modeling the grasp constraints, we present a particle filter-based framework to estimate the hand configuration. Specifically, given an arbitrary grasp, we start by sampling a set of hand configuration hypotheses and then randomly manipulate the object within the hand. While observing the object's movements as evidence using an external camera, which is not necessarily calibrated with the hand frame, our estimator calculates the likelihood of each hypothesis to iteratively estimate the hand configuration. Once converged, the estimator is used to track the hand configuration in real-time for future manipulations. Thereafter, we develop an algorithm to precisely plan and control the underactuated manipulation to move the grasped object to desired poses. In contrast to most other dexterous manipulation approaches, our framework does not require any tactile sensing or joint encoders and can directly operate on any novel objects without requiring a model of the object a priori. We implemented our framework on both the Yale Model O hand and the Yale T42 hand. The results show that the estimation is accurate for different objects and that the framework can be easily adapted across different underactuated hand models. Finally, we evaluated our planning and control algorithm with handwriting tasks and demonstrated the effectiveness of the proposed framework.","tags":["Source Themes"],"title":"Hand-object configuration estimation using particle filters for dexterous in-hand manipulation","type":"publication"},{"authors":["Yasemin Bekiroglu","Naresh Marturi","Maximo A. Roa","Maxime Adjigble","Tommaso Pardi","Cindy Grimm","Ravi Balasubramanian","Kaiyu Hang","Rustam Stolkin"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1565556060,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565556060,"objectID":"b2f3a0184b466a3cdc4189a8078849ec","permalink":"https://robotpilab.github.io/publication/benchmarking-protocol-for-grasp-planning-algorithms/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/benchmarking-protocol-for-grasp-planning-algorithms/","section":"publication","summary":"Numerous grasp planning algorithms have been proposed since the 1980s. The grasping literature has expanded rapidly in recent years, building on greatly improved vision systems and computing power. Methods have been proposed to plan stable grasps on known objects (exact 3D model is available), familiar objects (e.g. exploiting a-priori known grasps for different objects of the same category), or novel object shapes observed during task execution. Few of these methods have ever been compared in a systematic way, and objective performance evaluation of such complex systems remains problematic. Difficulties and confounding factors include different assumptions and amounts of a-priori knowledge in different algorithms, different robots, hands, vision systems, and setups in different labs, and different choices or application needs for grasped objects. Also, grasp planning can use different grasp quality metrics (including empirical or theoretical stability measures), or other criteria, e.g. computational speed, or combination of grasps with reachability considerations. While acknowledging and discussing the outstanding difficulties surrounding this complex topic, we propose a methodology for reproducible experiments to compare the performance of a variety of grasp planning algorithms. Our protocol attempts to improve the objectivity with which different grasp planners are compared by minimizing the influence of key components in the grasping pipeline, e.g. vision and pose estimation. The protocol is demonstrated by evaluating two different grasp planners: a state-of-the-art model-free planner and a popular open-source model-based planner. We show results from real-robot experiments with a 7-DoF arm and 2-finger hand and simulation-based evaluations.","tags":["Source Themes"],"title":"Benchmarking Protocol for Grasp Planning Algorithms","type":"publication"},{"authors":["Walter G. Bircher","Andrew S. Morgan","Kaiyu Hang","Aaron M. Dollar"],"categories":null,"content":"","date":1561919640,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561919640,"objectID":"8985257cd183807d3a90de058776527a","permalink":"https://robotpilab.github.io/publication/energy-gradient-based-graphs-for-planning-within-hand-caging-manipulation/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/energy-gradient-based-graphs-for-planning-within-hand-caging-manipulation/","section":"publication","summary":"In this work, we present a within-hand manipulation approach that leverages a simple energy model based on caging grasps made by underactuated hands. Instead of explicitly modeling the contacts and dynamics in manipulation, we can calculate a map to describe the energy states of different hand-object configurations under an actuation input. Since the system intrinsically steers towards low energy states, the object's movement is uniquely described by the gradient of the energy map if the corresponding actuation is applied. Such maps are pre-calculated for a range of actuation inputs to represent the system's energy profile. We discretize the workspace into a grid and construct an energy gradient-based graph by locally exploring the gradients of the stored energy profile. Given a goal configuration of a simple cylindrical object, a sequence of actuation inputs can be calculated to manipulate it towards the goal by exploiting the connectivity in the graph. The proposed approach is experimentally implemented on a Yale T42 hand. Our evaluation results show that parts of the graph are well-connected, explaining our ability to successfully plan and execute trajectories within the gripper's workspace.","tags":[],"title":"Energy Gradient-Based Graphs for Planning Within-Hand Caging Manipulation","type":"publication"},{"authors":["Kaiyu Hang","Andrew S. Morgan","Aaron M. Dollar"],"categories":null,"content":"","date":1561778820,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561778820,"objectID":"352507a763c2e43a39e2a414d10c060b","permalink":"https://robotpilab.github.io/publication/pre-grasp-sliding-manipulation-of-thin-objects-using-soft-compliant-or-underactuated-hands/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/pre-grasp-sliding-manipulation-of-thin-objects-using-soft-compliant-or-underactuated-hands/","section":"publication","summary":"Person re-identification is a critical security task for recognizing a person across spatially disjoint sensors. Previous work can be computationally intensive and is mainly based on low-level cues extracted from RGB data and implemented on a PC for a fixed sensor network (such as traditional CCTV). We present a practical and efficient framework for mobile devices, such as smartphones and robots, where high-level semantic soft biometrics are extracted from RGB and depth data. By combining these cues, our approach attempts to provide robustness to noise, illumination, and minor variations in clothing. This mobile approach may be particularly useful for identifying people in areas ill-served by fixed sensors or for tasks where the sensor position and direction need to dynamically adapt to a target. Results on the BIWI dataset are preliminary but encouraging. Further evaluation and demonstration of the system will be available on our website.","tags":[],"title":"Pre-Grasp Sliding Manipulation of Thin Objects Using Soft, Compliant, or Underactuated Hands","type":"publication"},{"authors":["Joshua A. Haustein","Silvia Cruciani","Rizwan Asif","Kaiyu Hang and Danica Kragic"],"categories":null,"content":"","date":1561778700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561778700,"objectID":"3fbee829135381a11575937d9171d956","permalink":"https://robotpilab.github.io/publication/placing-objects-with-prior-in-hand-manipulation-using-dexterous-manipulation-graphs/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/placing-objects-with-prior-in-hand-manipulation-using-dexterous-manipulation-graphs/","section":"publication","summary":"Person re-identification is a critical security task for recognizing a person across spatially disjoint sensors. Previous work can be computationally intensive and is mainly based on low-level cues extracted from RGB data and implemented on a PC for a fixed sensor network (such as traditional CCTV). We present a practical and efficient framework for mobile devices, such as smartphones and robots, where high-level semantic soft biometrics are extracted from RGB and depth data. By combining these cues, our approach attempts to provide robustness to noise, illumination, and minor variations in clothing. This mobile approach may be particularly useful for identifying people in areas ill-served by fixed sensors or for tasks where the sensor position and direction need to dynamically adapt to a target. Results on the BIWI dataset are preliminary but encouraging. Further evaluation and demonstration of the system will be available on our website.","tags":[],"title":"Placing Objects with prior In-Hand Manipulation using Dexterous Manipulation Graphs","type":"publication"},{"authors":["Silvia Cruciani","Kaiyu Hang","Christian Smith","Danica Kragic"],"categories":null,"content":"","date":1561778580,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561778580,"objectID":"cf1cbd68b406f6ef8aa020476d6a66be","permalink":"https://robotpilab.github.io/publication/dual-arm-in-hand-manipulation-using-visual-feedback/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/dual-arm-in-hand-manipulation-using-visual-feedback/","section":"publication","summary":"In this work, we address the problem of executing in-hand manipulation based on visual input. Given an initial grasp, the robot has to change its grasp configuration without releasing the object. We propose a method for in-hand manipulation planning and execution based on information on the object's shape using a dual-arm robot. From the available information on the object, which can be a complete point cloud but also partial data, our method plans a sequence of rotations and translations to reconfigure the object's pose. This sequence is executed using non-prehensile pushes defined as relative motions between the two robot arms.","tags":[],"title":"Dual-Arm In-Hand Manipulation Using Visual Feedback","type":"publication"},{"authors":["Joshua A. Haustein","Kaiyu Hang","Johannes A. Stork","Danica Kragic"],"categories":null,"content":"","date":1561761840,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561761840,"objectID":"86f015639d41b0ded05bae0f2a0ccd3d","permalink":"https://robotpilab.github.io/publication/object-placement-planning-and-optimization-for-robot-manipulators/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/object-placement-planning-and-optimization-for-robot-manipulators/","section":"publication","summary":"We propose an anytime algorithm to plan the placement of a rigid object with a dual-arm robot in a cluttered environment. The algorithm integrates sampling-based motion planning with a novel hierarchical search for suitable placement poses, incrementally producing approach motions to stable placement poses and reaching placements with better objectives as runtime progresses. Our approach is effective for two different placement objectives, even in challenging scenarios.","tags":[],"title":"Object Placement Planning and Optimization for Robot Manipulators","type":"publication"},{"authors":["Andrew S. Morgan","Kaiyu Hang","Walter G. Bircher","Aaron M. Dollar"],"categories":null,"content":"","date":1561761660,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561761660,"objectID":"d581633fe88a912570a79589ac2e2288","permalink":"https://robotpilab.github.io/publication/a-data-driven-framework-for-learning-dexterous-manipulation-of-unknown-objects/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/a-data-driven-framework-for-learning-dexterous-manipulation-of-unknown-objects/","section":"publication","summary":"The paper presents a method for developing precise, quasi-static control strategies for fingertip manipulation in robot hands. The method involves extracting object transition maps by tracking the state of the grasp frame and developing a regression map of the action-reaction pairs. The approach is agnostic to the global geometry of the object and is able to adapt when undesirable contact conditions occur. The methodology estimates the nonlinearities representative in the properties of the system. The framework is tested physically on an adapted Yale Openhand Model O and demonstrated to be robust to inaccuracies in parameter estimation.","tags":[],"title":"A Data-Driven Framework for Learning Dexterous Manipulation of Unknown Objects","type":"publication"},{"authors":["Kaiyu Hang","Ximin Lyu","Haoran Song","Johannes A. Stork","Aaron M. Dollar","Danica Kragic","Fu Zhang"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1552337160,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552337160,"objectID":"fdd4d6cec18e8a8539ffc5259011d938","permalink":"https://robotpilab.github.io/publication/perching-and-resting-a-paradigm-for-uav-maneuvering-with-modularized-landing-gears/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/perching-and-resting-a-paradigm-for-uav-maneuvering-with-modularized-landing-gears/","section":"publication","summary":"Perching helps small unmanned aerial vehicles (UAVs) extend their time of operation by saving battery power. However, most strategies for UAV perching require complex maneuvering and rely on specific structures, such as rough walls for attaching or tree branches for grasping. Many strategies to perching neglect the UAV’s mission such that saving battery power interrupts the mission. We suggest enabling UAVs with the capability of making and stabilizing contacts with the environment, which will allow the UAV to consume less energy while retaining its altitude, in addition to the perching capability that has been proposed before. This new capability is termed “resting.” For this, we propose a modularized and actuated landing gear framework that allows stabilizing the UAV on a wide range of different structures by perching and resting. Modularization allows our framework to adapt to specific structures for resting through rapid prototyping with additive manufacturing. Actuation allows switching between different modes of perching and resting during flight and additionally enables perching by grasping. Our results show that this framework can be used to perform UAV perching and resting on a set of common structures, such as street lights and edges or corners of buildings. We show that the design is effective in reducing power consumption, promotes increased pose stability, and preserves large vision ranges while perching or resting at heights. In addition, we discuss the potential applications facilitated by our design, as well as the potential issues to be addressed for deployment in practice.","tags":["Source Themes"],"title":"Perching and resting -- A paradigm for UAV maneuvering with modularized landing gears","type":"publication"},{"authors":["Haoran Song","Michael Y. Wang","Kaiyu Hang"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1534020780,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534020780,"objectID":"35d90aa95222e0aadf8e5f69837c476d","permalink":"https://robotpilab.github.io/publication/fingertip-surface/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/fingertip-surface/","section":"publication","summary":"We address the problem of designing fingertips by leveraging the fact that most grasp contacts share a few classes of local geometries. In order to maximize the contact areas and achieve more robust grasps, we first define the concept of a Contact Primitive, which represents a set of contacts with similar local geometries. Thereafter, we propose a uniform cost algorithm, formulated as a decision-making process in a tree structure, to cluster a set of example grasp contacts into a finite set of Contact Primitives. We design fingertips by optimizing to match the local geometry of each contact primitive and then 3D print them using soft materials to compensate for optimization residuals. For novel objects, we provide an efficient algorithm to generate grasp contacts that match the fingertip geometries while forming stable grasps. Comparing to a baseline of flat fingertip design, the experimental results show that our design significantly improves grasp stability and is more robust against various uncertainties.","tags":["Source Themes"],"title":"Fingertip Surface Optimization for Robust Grasping on Contact Primitives","type":"publication"},{"authors":["Haoran Song","Michael Y. Wang","Kaiyu Hang"],"categories":null,"content":"","date":1530384720,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530384720,"objectID":"e02e751a2f947eb16d18ccbeaecb21b4","permalink":"https://robotpilab.github.io/publication/fingertip-surface-optimization-for-robust-grasping-on-contact-primitives/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/fingertip-surface-optimization-for-robust-grasping-on-contact-primitives/","section":"publication","summary":"We address the problem of fingertip design by leveraging the fact that most grasp contacts share a few classes of local geometries. To maximize the contact areas for achieving more robust grasps, we first define the concept of a Contact Primitive, which represents a set of contacts with similar local geometries. Thereafter, we propose a uniform cost algorithm, formulated as a decision-making process in a tree structure, to cluster a set of example grasp contacts into a finite set of Contact Primitives. We design fingertips by optimization to match the local geometry of each contact primitive, and then 3D print them using soft materials to compensate for optimization residuals. For novel objects, we provide an efficient algorithm to generate grasp contacts that match the fingertip geometries while forming stable grasps. Comparing to a baseline of flat fingertip design, the experimental results show that our design significantly improves grasp stability and is more robust against various uncertainties.","tags":[],"title":"Fingertip Surface Optimization for Robust Grasping on Contact Primitives","type":"publication"},{"authors":["Weihao Yuan","Johannes A. Stork","Danica Kragic","Michael Y. Wang and Kaiyu Hang"],"categories":null,"content":"","date":1530384660,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530384660,"objectID":"04153887589a9808c74265b9a73c4da9","permalink":"https://robotpilab.github.io/publication/rearrangement-with-nonprehensile-manipulation-using-deep-reinforcement-learning/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/rearrangement-with-nonprehensile-manipulation-using-deep-reinforcement-learning/","section":"publication","summary":"Rearranging objects on a tabletop surface by means of nonprehensile manipulation is a task that requires skillful interaction with the physical world. Usually, this is achieved by precisely modeling the physical properties of the objects, robot, and environment for explicit planning. In contrast, explicitly modeling the physical environment is not always feasible and involves various uncertainties. Therefore, we learn a nonprehensile rearrangement strategy with deep reinforcement learning based on only visual feedback. To do this, we model the task with rewards and train a deep Q-network. Our potential field-based heuristic exploration strategy reduces the number of collisions that lead to suboptimal outcomes, and we actively balance the training set to avoid bias towards poor examples. Our training process leads to quicker learning and better performance on the task compared to uniform exploration and standard experience replay. We demonstrate empirical evidence from simulation that our method leads to a success rate of 85%. We also show that our system can cope with sudden changes in the environment and compare our performance with human-level performance.","tags":[],"title":"Rearrangement with Nonprehensile Manipulation Using Deep Reinforcement Learning","type":"publication"},{"authors":["Silvia Cruciani","Christian Smith","Danica Kragic","Kaiyu Hang"],"categories":null,"content":"","date":1530383820,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530383820,"objectID":"0e5e8e6a983657868febb080e553504d","permalink":"https://robotpilab.github.io/publication/dexterous-manipulation-graphs/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/dexterous-manipulation-graphs/","section":"publication","summary":"We propose the Dexterous Manipulation Graph as a tool to address in-hand manipulation and reposition an object inside a robot's end-effector. This graph is used to plan a sequence of manipulation primitives to bring the object to the desired end pose. This sequence of primitives is translated into motions of the robot to move the object held by the end-effector. We use a dual-arm robot with parallel grippers to test our method on a real system and show successful planning and execution of in-hand manipulation.","tags":[],"title":"Dexterous Manipulation Graphs","type":"publication"},{"authors":["Berk Calli","Andrew Kimmel","Kaiyu Hang","Kostas Bekris","and Aaron M. Dollar"],"categories":null,"content":"","date":1530383760,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530383760,"objectID":"235f7891343b2997c03f9277321a2ad4","permalink":"https://robotpilab.github.io/publication/path-planning-for-within-hand-manipulation-over-learned-representations-of-safe-states/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/path-planning-for-within-hand-manipulation-over-learned-representations-of-safe-states/","section":"publication","summary":"This work proposes a framework for tracking a desired path of an object held by an adaptive hand via within-hand manipulation. Such underactuated hands are able to passively achieve stable contacts with objects. Combined with vision-based control and data-driven state estimation processes, they can solve tasks without accurate hand-object models or multimodal sensory feedback. In particular, a data-driven regression process is used here to estimate the probability of dropping the object for given manipulation states. Then, an optimization-based planner aims to track the desired path while avoiding states that are above a threshold probability of dropping the object. The optimized cost function, based on the principle of Dynamic Time Warping (DTW), seeks to minimize the area between the desired and the followed path. By adapting the threshold for the probability of dropping the object, the framework can handle objects of different weights without retraining. Experiments involving writing letters with a marker, as well as tracing randomized paths, were conducted on the Yale Model T-42 hand. Results indicate that the framework successfully avoids undesirable states while minimizing the proposed cost function, thereby producing object paths for within-hand manipulation that closely match the target ones.","tags":[],"title":"Path Planning for Within-Hand Manipulation over Learned Representations of Safe States","type":"publication"},{"authors":["Kaiyu Hang","Johannes A. Stork","Nancy S. Pollard","Danica Kragic"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1502484840,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1502484840,"objectID":"ee5af8aa49ed1aabacb8e2e4c7c4815d","permalink":"https://robotpilab.github.io/publication/a-framework/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/a-framework/","section":"publication","summary":"We consider the problem of finding optimal grasp contacts for arbitrary objects, based on a given grasp quality function. Our approach formulates a framework for contact-level grasping as a path-finding problem in the space of super-contact grasps. The initial super-contact grasp contains all grasps, and in each step along a path, grasps are removed. To achieve this, we introduce and formally characterize search space structure and cost functions, under which minimal cost paths correspond to optimal grasps. Our formulation avoids expensive exhaustive search and reduces computational cost by several orders of magnitude. \n\nWe present admissible heuristic functions and exploit approximate heuristic search to further reduce computational cost while maintaining bounded sub-optimality for resulting grasps. We exemplify our formulation with point-contact grasping, for which we define domain-specific heuristics and demonstrate optimality and bounded sub-optimality by comparing against exhaustive and uniform cost search on example objects. Furthermore, we explain how to restrict the search graph to satisfy grasp constraints for modeling hand kinematics. We also analyze our algorithm empirically in terms of created and visited search states and resultant effective branching factor.","tags":["Source Themes"],"title":"A Framework For Optimal Grasp Contact Planning","type":"publication"},{"authors":["Kaiyu Hang","Johannes A. Stork","Nancy S. Pollard","Danica Kragic"],"categories":null,"content":"","date":1498849020,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498849020,"objectID":"a017db0ecc8e7e07f7c02d84a81e7c01","permalink":"https://robotpilab.github.io/publication/a-framework-for-optimal-grasp-contact-planning/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/a-framework-for-optimal-grasp-contact-planning/","section":"publication","summary":"We consider the problem of finding optimal grasp contacts for arbitrary objects, based on a given grasp quality function. Our approach formulates a framework for contact-level grasping as a path-finding problem in the space of super-contact grasps. The initial super-contact grasp contains all grasps, and in each step along a path, grasps are removed. To achieve this, we introduce and formally characterize search space structure and cost functions, under which minimal cost paths correspond to optimal grasps. Our formulation avoids expensive exhaustive search and reduces computational cost by several orders of magnitude. We present admissible heuristic functions and exploit approximate heuristic search to further reduce computational cost while maintaining bounded sub-optimality for resulting grasps.\n\nWe exemplify our formulation with point-contact grasping, for which we define domain-specific heuristics and demonstrate optimality and bounded sub-optimality by comparing against exhaustive and uniform cost search on example objects. Furthermore, we explain how to restrict the search graph to satisfy grasp constraints for modeling hand kinematics. We also analyze our algorithm empirically in terms of created and visited search states and resultant effective branching factor.","tags":[],"title":"A Framework For Optimal Grasp Contact Planning","type":"publication"},{"authors":["Joshua A. Haustein","Kaiyu Hang and Danica Kragic"],"categories":null,"content":"","date":1498848900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498848900,"objectID":"0ae95f345a808263f73736bba9e20463","permalink":"https://robotpilab.github.io/publication/integrating-motion-and-hierarchical-fingertip-grasp-planning/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/integrating-motion-and-hierarchical-fingertip-grasp-planning/","section":"publication","summary":"In this work, we present an algorithm that simultaneously searches for a high-quality fingertip grasp and a collision-free path for a robot hand-arm system to achieve it. The algorithm combines a bidirectional sampling-based motion planning approach with a hierarchical contact optimization process. Instead of tackling these problems in a decoupled manner, the grasp optimization is guided by the proximity to collision-free configurations explored by the motion planner. We implemented the algorithm for a 13-DoF manipulator and showed that it is capable of efficiently planning reachable high-quality grasps in cluttered environments. Furthermore, we demonstrated that our algorithm outperforms a decoupled integration in terms of planning runtime.","tags":[],"title":"Integrating Motion and Hierarchical Fingertip Grasp Planning","type":"publication"},{"authors":["Anastasiia Varava","Kaiyu Hang","Danica Kragic","Florian T. Pokorny"],"categories":null,"content":"","date":1498848840,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498848840,"objectID":"19187fb100ae47978d07ba6c28066bcd","permalink":"https://robotpilab.github.io/publication/herding-by-caging-a-topological-approach-towards-guiding-moving-agents-via-mobile-robots/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/herding-by-caging-a-topological-approach-towards-guiding-moving-agents-via-mobile-robots/","section":"publication","summary":"In this paper, we propose a solution to the problem of herding by caging. Given a set of mobile robots (called herders) and a group of moving agents (called sheep), we move the latter to a predefined location in such a way that they cannot escape from the robots while moving. We model the interaction between the herders and the sheep by assuming that the former exert virtual \"repulsive forces\" that push the sheep away from them. These forces induce a potential field in which the sheep move in a way that does not increase their potential. This enables the robots to partially control the motion of the sheep. We formalize this behavior geometrically by applying the notion of caging, which is widely used in robotic grasping. We show that our approach is provably correct in the sense that the sheep cannot escape from the robots. We propose an RRT-based motion planning algorithm, demonstrate its probabilistic completeness, and evaluate it in simulations.","tags":[],"title":"Herding by Caging: A Topological Approach towards Guiding Moving Agents via Mobile Robots","type":"publication"},{"authors":["Miao Li","Kaiyu Hang","Danica Kragic","Aude Billard"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1470949080,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470949080,"objectID":"73becc134f08f03fb81192485d7732f2","permalink":"https://robotpilab.github.io/publication/dexterous-grasping-under-shape-uncertainty/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/dexterous-grasping-under-shape-uncertainty/","section":"publication","summary":"An important challenge in robotics is achieving robust performance in object grasping and manipulation while dealing with noise and uncertainty. This paper presents an approach for addressing the performance of dexterous grasping under shape uncertainty. In our approach, the uncertainty in the object's shape is parameterized and incorporated as a constraint into grasp planning. The proposed approach is used to plan feasible hand configurations for realizing planned contacts using different robotic hands. A compliant finger closing scheme is devised by exploiting both the object's shape uncertainty and tactile sensing at the fingertips. Experimental evaluation demonstrates that our method improves the performance of dexterous grasping under shape uncertainty.","tags":["Source Themes"],"title":"Dexterous Grasping under Shape Uncertainty","type":"publication"},{"authors":["Kaiyu Hang","Miao Li","Johannes A. Stork","Yasemin Bekiroglu","Florian T. Pokorny","Aude Billard","Danica Kragic"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1470948960,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470948960,"objectID":"64507d0bb241d243c5a2df3716fc977a","permalink":"https://robotpilab.github.io/publication/hierarchical-fingertip-space-a-unified-framework-for-grasp-planning-and-in-hand-grasp-adaptation/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/hierarchical-fingertip-space-a-unified-framework-for-grasp-planning-and-in-hand-grasp-adaptation/","section":"publication","summary":"We present a unified framework for grasp planning and in-hand grasp adaptation using visual, tactile, and proprioceptive feedback. The main objective of the proposed framework is to enable fingertip grasping by addressing problems such as changes in the weight of the object, slippage, and external disturbances. For this purpose, we introduce the Hierarchical Fingertip Space (HFTS) as a representation that enables optimization for both efficient grasp synthesis and online finger gaiting. Grasp synthesis is followed by a grasp adaptation step that consists of both grasp force adaptation through impedance control and regrasping/finger gaiting when the former is not sufficient. Experimental evaluation is conducted on an Allegro hand mounted on a Kuka LWR arm.","tags":["Source Themes"],"title":"Hierarchical Fingertip Space: A Unified Framework for Grasp Planning and In-Hand Grasp Adaptation","type":"publication"},{"authors":["Kaiyu Hang","Joshua A. Haustein","Miao Li","Aude Billard","Christian Smith","Danica Kragic"],"categories":null,"content":"","date":1467313080,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1467313080,"objectID":"296cba22ad8471e24933d273c89ee09f","permalink":"https://robotpilab.github.io/publication/on-the-evolution-of-fingertip-grasping-manifolds/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/on-the-evolution-of-fingertip-grasping-manifolds/","section":"publication","summary":"Efficient and accurate planning of fingertip grasps is essential for dexterous in-hand manipulation. In this work, we present a system for fingertip grasp planning that incrementally learns a heuristic for hand reachability and multi-fingered inverse kinematics. The system consists of an online execution module and an offline optimization module. During execution, the system plans and executes fingertip grasps using Canny's grasp quality metric and a learned random forest-based hand reachability heuristic. In the offline module, this heuristic is improved based on a grasping manifold that is incrementally learned from the experiences collected during execution. The system is evaluated both in simulation and on a SchunkSDH dexterous hand mounted on a KUKA-KR5 arm. We show that, as the grasping manifold is adapted to the system's experiences, the heuristic becomes more accurate, resulting in improved performance of the execution module. The improvement is not only observed for experienced objects but also for previously unknown objects of similar sizes.","tags":[],"title":"On the Evolution of Fingertip Grasping Manifolds","type":"publication"},{"authors":["Kaiyu Hang","Johannes A. Stork","Florian T. Pokorny","Danica Kragic"],"categories":null,"content":"","date":1404154920,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1404154920,"objectID":"d038d0412c73a19b290909386c32d76f","permalink":"https://robotpilab.github.io/publication/combinatorial-optimization-for-hierarchical-contact-level-grasping/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/combinatorial-optimization-for-hierarchical-contact-level-grasping/","section":"publication","summary":"We address the problem of generating force-closed point contact grasps on complex surfaces and model it as a combinatorial optimization problem. Using a multilevel refinement metaheuristic, we maximize the quality of a grasp subject to a reachability constraint by recursively forming a hierarchy of increasingly coarser optimization problems. A grasp is initialized at the top of the hierarchy and then locally refined until convergence at each level. Our approach efficiently addresses the high-dimensional problem of synthesizing stable point contact grasps while resulting in stable grasps from arbitrary initial configurations. Compared to a sampling-based approach, our method yields grasps with higher quality. Empirical results are presented for a set of different objects. We investigate the number of levels in the hierarchy, the computational complexity, and the performance relative to a random sampling baseline approach.","tags":[],"title":"Combinatorial Optimization for Hierarchical Contact-level Grasping","type":"publication"},{"authors":["Kaiyu Hang","Johannes A. Stork","Danica Kragic"],"categories":null,"content":"","date":1404154800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1404154800,"objectID":"43b4b8a1374b0c6675c6a0b821feae51","permalink":"https://robotpilab.github.io/publication/hierarchical-fingertip-space-for-multi-fingered-precision-grasping/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/hierarchical-fingertip-space-for-multi-fingered-precision-grasping/","section":"publication","summary":"Dexterous manipulation of objects by a robot system benefits from the ability to generate precise grasps. In this paper, we propose the concept of Fingertip Space and its use for synthesizing precision grasps. Fingertip Space is a representation that takes into account both the local geometry of the object surface and the fingertip geometry. As such, it is directly applicable to the object point cloud data and establishes a basis for the grasp search space. We propose a model for hierarchically encoding the Fingertip Space that enables multilevel refinement for efficient grasp synthesis. The proposed method works at the grasp contact level while not neglecting object shape or hand kinematics. Experimental evaluation is performed for the Barrett hand, considering noisy and incomplete point cloud data.","tags":[],"title":"Hierarchical Fingertip Space for Multi-fingered Precision Grasping","type":"publication"},{"authors":["Florian T. Pokorny","Kaiyu Hang","Danica Kragic"],"categories":null,"content":"","date":1372619040,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372619040,"objectID":"ac6f62d36b38247cf4961e05b4e7036f","permalink":"https://robotpilab.github.io/publication/grasp-moduli-spaces/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/grasp-moduli-spaces/","section":"publication","summary":"We present a new approach for modeling grasping using an integrated space of grasps and shapes. In particular, we introduce an infinite-dimensional space, the Grasp Moduli Space, which represents shapes and grasps in a continuous manner. We define a metric on this space that allows us to formalize \"nearby\" grasp/shape configurations, and we discuss continuous deformations of such configurations. We work in particular with surfaces with cylindrical coordinates and analyze the stability of a popular L1 grasp quality measure Ql under continuous deformations of shapes and grasps. We experimentally determine bounds on the maximal change of Ql in a small neighborhood around stable grasps with grasp quality above a threshold. In the case of surfaces of revolution, we determine stable grasps that correspond to grasps used by humans and develop an efficient algorithm for generating those grasps in the case of three contact points. We show that sufficiently stable grasps stay stable under small deformations. For larger deformations, we develop a gradient-based method that can transfer stable grasps between different surfaces. Additionally, we show in experiments that our gradient method can be used to find stable grasps on arbitrary surfaces with cylindrical coordinates by deforming such surfaces towards a corresponding \"canonical\" surface of revolution.","tags":[],"title":"Grasp Moduli Spaces","type":"publication"},{"authors":["Kaiyu Hang","Florian T. Pokorny","Danica Kragic"],"categories":null,"content":"","date":1372618980,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372618980,"objectID":"e418502128522d1cf5102f590a2fffd3","permalink":"https://robotpilab.github.io/publication/friction-coefficients-and-grasp-synthesis/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/friction-coefficients-and-grasp-synthesis/","section":"publication","summary":"We propose a new concept called \"friction sensitivity,\" which measures how susceptible a specific grasp is to changes in the underlying friction coefficients. We develop algorithms for synthesizing stable grasps with low friction sensitivity, as well as for synthesizing stable grasps in the case of small friction coefficients. We describe how grasps with low friction sensitivity can be used when a robot has an uncertain belief about friction coefficients, and study the statistics of grasp quality under changes in those coefficients. Additionally, we provide a parametric estimate for the distribution of grasp qualities and friction sensitivities for a uniformly sampled set of grasps.","tags":[],"title":"Friction Coefficients and Grasp Synthesis","type":"publication"},{"authors":["Marianna Madry","Carl Henrik Ek","Renaud Detry","Kaiyu Hang","Danica Kragic"],"categories":null,"content":"","date":1341083160,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1341083160,"objectID":"3a8e5c1a2dfd8a6f8ad6f481e341cefc","permalink":"https://robotpilab.github.io/publication/improving-generalization-for-3d-object-categorization-with-global-structure-histograms/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/improving-generalization-for-3d-object-categorization-with-global-structure-histograms/","section":"publication","summary":"We propose a new object descriptor for three-dimensional data called the Global Structure Histogram (GSH). The GSH encodes the structure of a local feature response on a coarse global scale, providing a beneficial trade-off between generalization and discrimination. Encoding the structural characteristics of an object allows us to retain low local variations while keeping the benefit of global representativeness. In an extensive experimental evaluation, we applied the framework to category-based object classification in realistic scenarios. We show results obtained by combining the GSH with several different local shape representations, and we demonstrate significant improvements compared to other state-of-the-art global descriptors.","tags":[],"title":"Improving Generalization for 3D Object Categorization with Global Structure Histograms","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bb238c2d1697da0a6715e7e8c8299cef","permalink":"https://robotpilab.github.io/project/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/","section":"","summary":"","tags":null,"title":"Projects","type":"landing"}]