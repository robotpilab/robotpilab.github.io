---
title: Manipulation for self-Identification, and self-Identification for better
  manipulation
subtitle: Top 5 Editor's Picks of 2021
publication_types:
  - "2"
authors:
  - Kaiyu Hang
  - Walter G. Bircher
  - Andrew S. Morgan
  - Aaron M. Dollar
publication_short: Science Robotics
abstract: The process of modeling a series of hand-object parameters is crucial
  for precise and controllable robotic in-hand manipulation because it enables
  the mapping from the hand’s actuation input to the object’s motion to be
  ob-tained. Without assuming that most of these model parameters are known a
  priori or can be easily estimated by sensors, we focus on equipping robots
  with the ability to actively self-identify necessary model parameters using
  minimal sensing. Here, we derive algorithms, on the basis of the concept of
  virtual linkage-based representations (VLRs), to self-identify the underlying
  mechanics of hand-object systems via exploratory manipulation actions and
  probabilistic reasoning and, in turn, show that the self-identified VLR can
  enable the control of precise in-hand ma-nipulation. To validate our
  framework, we instantiated the proposed system on a Yale Model O hand without
  joint
  encoders  or  tactile  sensors.  The  passive  adaptability  of  the  underactuated  hand  greatly  facilitates  the  self-identification  process,
  because they naturally secure stable hand-object interactions during random
  exploration. Relying solely on an in-hand camera, our system can effectively
  self-identify the VLRs, even when some fingers are replaced with novel
  designs. In addition, we show in-hand manipulation applications of
  handwriting, marble maze playing, and cup stacking to demonstrate the
  effectiveness of the VLR in precise in-hand manipulation control.
draft: false
featured: false
projects: []
slides: null
url_pdf: https://hangkaiyu.github.io/self-identification.html
summary: The process of modeling a series of hand-object parameters is crucial
  for precise and controllable robotic in-hand manipulation because it enables
  the mapping from the hand’s actuation input to the object’s motion to be
  ob-tained. Without assuming that most of these model parameters are known a
  priori or can be easily estimated by sensors, we focus on equipping robots
  with the ability to actively self-identify necessary model parameters using
  minimal sensing. Here, we derive algorithms, on the basis of the concept of
  virtual linkage-based representations (VLRs), to self-identify the underlying
  mechanics of hand-object systems via exploratory manipulation actions and
  probabilistic reasoning and, in turn, show that the self-identified VLR can
  enable the control of precise in-hand ma-nipulation. To validate our
  framework, we instantiated the proposed system on a Yale Model O hand without
  joint
  encoders  or  tactile  sensors.  The  passive  adaptability  of  the  underactuated  hand  greatly  facilitates  the  self-identification  process,
  because they naturally secure stable hand-object interactions during random
  exploration. Relying solely on an in-hand camera, our system can effectively
  self-identify the VLRs, even when some fingers are replaced with novel
  designs. In addition, we show in-hand manipulation applications of
  handwriting, marble maze playing, and cup stacking to demonstrate the
  effectiveness of the VLR in precise in-hand manipulation control.
url_dataset: ""
url_project: ""
url_source: ""
url_video: ""
author_notes: []
doi: ""
publication: Science Robotics, 2021
tags:
  - Source Themes
image:
  caption: "Image credit: [**Unsplash**](https://unsplash.com/photos/jdD8gXaTZsc)"
  focal_point: ""
  preview_only: false
date: 2021-05-11T18:51:00.000Z
url_slides: ""
publishDate: 2017-01-01T00:00:00.000Z
url_poster: ""
url_code: ""
---

Supplementary notes can be added here, including [code and math](https://wowchemy.com/docs/content/writing-markdown-latex/).
