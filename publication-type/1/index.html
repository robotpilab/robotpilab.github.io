<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><script src=/js/mathjax-config.js></script><link rel=stylesheet href=/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=stylesheet href=/css/wowchemy.73de5f49e1703281b64ee4daa75626f8.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=description content="A highly-customizable Hugo research group theme powered by Wowchemy website builder."><link rel=alternate hreflang=en-us href=https://robotpilab.github.io/publication-type/1/><link rel=canonical href=https://robotpilab.github.io/publication-type/1/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu741267303ece0b55432a717ce280a15e_5637_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu741267303ece0b55432a717ce280a15e_5637_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="twitter:image" content="https://robotpilab.github.io/media/icon_hu741267303ece0b55432a717ce280a15e_5637_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="Rice RobotPi Lab"><meta property="og:url" content="https://robotpilab.github.io/publication-type/1/"><meta property="og:title" content="1 | Rice RobotPi Lab"><meta property="og:description" content="A highly-customizable Hugo research group theme powered by Wowchemy website builder."><meta property="og:image" content="https://robotpilab.github.io/media/icon_hu741267303ece0b55432a717ce280a15e_5637_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2023-07-28T17:55:21+00:00"><link rel=alternate href=/publication-type/1/index.xml type=application/rss+xml title="Rice RobotPi Lab"><title>1 | Rice RobotPi Lab</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Rice RobotPi Lab</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Rice RobotPi Lab</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/project><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/post><span>News</span></a></li><li class=nav-item><a class=nav-link href=/people><span>People</span></a></li><li class=nav-item><a class=nav-link href=/publication><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/contact><span>Contact</span></a></li><li class=nav-item><a class=nav-link href=/join><span>Join</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span>
</a><a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span>
</a><a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class="universal-wrapper pt-3"><h1>1</h1></div><div class=universal-wrapper><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/pod/>Non-Parametric Self-Identification and Model Predictive Control of Dexterous In-Hand Manipulation</a></div><a href=/publication/pod/ class=summary-link><div class=article-style>Building hand-object models for dexterous in-hand manipulation remains a crucial and open problem. Major challenges include the difficulty of obtaining the geometric and dynamic models of the hand, object, and time-varying contacts, as well as the inevitable physical and perceptual uncertainties. Instead of building accurate models to map between the actuation inputs and the object motions, this work proposes enabling the hand-object systems to continuously approximate their local models via a self-identification process where an underlying manipulation model is estimated through a small number of exploratory actions and non-parametric learning. With a very small number of data points, as opposed to most data-driven methods, our system self-identifies the underlying manipulation models online through exploratory actions and non-parametric learning. By integrating the self-identified hand-object model into a model predictive control framework, the proposed system closes the control loop to provide high accuracy in-hand manipulation. Furthermore, the proposed self-identification can adaptively trigger online updates through additional exploratory actions as soon as the self-identified local models render large discrepancies against the observed manipulation outcomes. We implemented the proposed approach on a sensorless underactuated Yale Model O hand with a single external camera to observe the object&rsquo;s motion. With extensive experiments, we show that the proposed self-identification approach can enable accurate and robust dexterous manipulation without requiring an accurate system model or a large amount of data for offline training.</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/podshara-chanrungmaneekul/>Podshara Chanrungmaneekul</a></span>, <span><a href=/author/kejia-ren/>Kejia Ren</a></span>, <span><a href=/author/joshua-t.-grace/>Joshua T. Grace</a></span>, <span><a href=/author/aaron-m.-dollar/>Aaron M. Dollar</a></span>, <span><a href=/author/kaiyu-hang/>Kaiyu Hang</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/2307.10033.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/pod/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://youtu.be/4FQ2193q1kk target=_blank rel=noopener>Video
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2307.10033 target=_blank rel=noopener>Arxiv</a></div></div><div class=ml-3><a href=/publication/pod/><img src=/publication/pod/featured_hu4b44b95634586fa56c227377f214a2bb_361502_150x0_resize_q75_h2_lanczos_3.webp height=112 width=150 alt="Non-Parametric Self-Identification and Model Predictive Control of Dexterous In-Hand Manipulation" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/kinodynamic-rapidly-exploring-random-forest-for-rearrangement-based-nonprehensile-manipulation/>Kinodynamic Rapidly-exploring Random Forest for Rearrangement-Based Nonprehensile Manipulation</a></div><a href=/publication/kinodynamic-rapidly-exploring-random-forest-for-rearrangement-based-nonprehensile-manipulation/ class=summary-link><div class=article-style>Our approach combines local rearrangement and global action optimization for nonprehensile manipulation, with free-space transit motions between constrained rearranging actions. We use a kinodynamic planning framework to search in multiple regions, allowing for global exploration of relevant subspaces and effective switches between local actions. The framework can adapt to real-world uncertainties and improves efficiency and effectiveness while remaining robust against uncertainties, as shown by extensive experiments.</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/kejia-ren/>Kejia Ren</a></span>, <span><a href=/author/podshara-chanrungmaneekul/>Podshara Chanrungmaneekul</a></span>, <span><a href=/author/lydia-e.-kavraki/>Lydia E. Kavraki</a></span>, <span><a href=/author/kaiyu-hang/>Kaiyu Hang</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/2302.04360.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/kinodynamic-rapidly-exploring-random-forest-for-rearrangement-based-nonprehensile-manipulation/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://youtu.be/xf6N-a95YKQ target=_blank rel=noopener>Video
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2302.04360 target=_blank rel=noopener>Arxiv</a></div></div><div class=ml-3><a href=/publication/kinodynamic-rapidly-exploring-random-forest-for-rearrangement-based-nonprehensile-manipulation/><img src=/publication/kinodynamic-rapidly-exploring-random-forest-for-rearrangement-based-nonprehensile-manipulation/featured_hubb2b140df93a1c5132eee4c980e41483_455962_150x0_resize_q75_h2_lanczos_3.webp height=104 width=150 alt="Kinodynamic Rapidly-exploring Random Forest for Rearrangement-Based Nonprehensile Manipulation" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/self-supervised-unseen-object-instance-segmentation-via-long-term-robot-interaction/>Self-Supervised Unseen Object Instance Segmentation via Long-Term Robot Interaction</a></div><a href=/publication/self-supervised-unseen-object-instance-segmentation-via-long-term-robot-interaction/ class=summary-link><div class=article-style>Our robot system improves segmentation of unseen object instances in the real world using multi-object tracking and video object segmentation on images collected via robot pushing. This self-supervised approach generates segmentation masks of all objects in the images, including those where objects are close together and segmentation errors are common. We fine-tune segmentation networks trained on synthetic data with real-world data collected by our system, significantly improving accuracy across domains and enhancing robotic grasping of previously unseen objects.</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/yangxiao-lu/>Yangxiao Lu</a></span>, <span><a href=/author/ninad-khargonkar/>Ninad Khargonkar</a></span>, <span><a href=/author/zesheng-xu/>Zesheng Xu</a></span>, <span><a href=/author/charles-averill/>Charles Averill</a></span>, <span><a href=/author/kamalesh-palanisamy/>Kamalesh Palanisamy</a></span>, <span><a href=/author/kaiyu-hang/>Kaiyu Hang</a></span>, <span><a href=/author/yunhui-guo/>Yunhui Guo</a></span>, <span><a href=/author/nicholas-ruozzi/>Nicholas Ruozzi</a></span>, <span><a href=/author/yu-xiang/>Yu Xiang</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2302.03793 target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://youtu.be/_ykvsRAXRT0 target=_blank rel=noopener>Video</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/conference-paper/>Rearrangement-Based Manipulation via Kinodynamic Planning and Dynamic Planning Horizons</a></div><a href=/publication/conference-paper/ class=summary-link><div class=article-style>This paper presents a planning framework for robot manipulation in cluttered environments that dynamically controls the planning horizon and interleaves planning and execution. The framework allows for flexible interaction with all objects and improves planning efficiency, robustness against physical uncertainties, and task success rate under limited time budgets.</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/kejia-ren/>Kejia Ren</a></span>, <span><a href=/author/lydia-e.-kavraki/>Lydia E. Kavraki</a></span>, <span><a href=/author/kaiyu-hang/>Kaiyu Hang</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2208.02312 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/conference-paper/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://youtu.be/8xgkMSeqCts target=_blank rel=noopener>Video</a></div></div><div class=ml-3><a href=/publication/conference-paper/><img src=/publication/conference-paper/featured_hu37f242c57ef3ce40b91839c0282958cf_405590_150x0_resize_q75_h2_lanczos_3.webp height=139 width=150 alt="Rearrangement-Based Manipulation via Kinodynamic Planning and Dynamic Planning Horizons" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/multi-object-rearrangement-with-monte-carlo-tree-search-a-case-study-on-planar-nonprehensile-sorting/>Multi-Object Rearrangement with Monte Carlo Tree Search: A Case Study on Planar Nonprehensile Sorting</a></div><a href=/publication/multi-object-rearrangement-with-monte-carlo-tree-search-a-case-study-on-planar-nonprehensile-sorting/ class=summary-link><div class=article-style>The article proposes using Monte Carlo tree search with a task-specific heuristic function to solve a planar non-prehensile sorting task where a robot needs to push densely packed objects into a configuration where classes are separated. The algorithm is evaluated on various simulated and real-world sorting tasks and is capable of reliably sorting large numbers of objects, including convex and non-convex objects and convex objects in the presence of immovable obstacles.</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/haoran-song/>Haoran Song</a></span>, <span><a href=/author/joshua-a.-haustein/>Joshua A. Haustein</a></span>, <span><a href=/author/weihao-yuan/>Weihao Yuan</a></span>, <span><a href=/author/kaiyu-hang/>Kaiyu Hang</a></span>, <span><a href=/author/michael-y.-wang/>Michael Y. Wang</a></span>, <span><a href=/author/danica-kragic/>Danica Kragic</a></span>, <span><a href=/author/johannes-a.-stork/>Johannes A. Stork</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://hangkaiyu.github.io/pdfs/song2020a.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/multi-object-rearrangement-with-monte-carlo-tree-search-a-case-study-on-planar-nonprehensile-sorting/cite.bib>Cite</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/reinforcement-learning-in-topology-based-representation-for-human-body-movement-with-whole-arm-manipulation/>Reinforcement Learning in Topology-based Representation for Human Body Movement with Whole Arm Manipulation</a></div><a href=/publication/reinforcement-learning-in-topology-based-representation-for-human-body-movement-with-whole-arm-manipulation/ class=summary-link><div class=article-style>Moving a human body or a large and bulky object may require the strength of whole-arm manipulation (WAM). This type of manipulation places the load on the robot&rsquo;s arms and relies on global properties of the interaction to succeed—rather than local contacts such as grasping or non-prehensile pushing. In this paper, we learn to generate motions that enable WAM for holding and transporting humans in certain rescue or patient care scenarios. We model the task as a reinforcement learning problem to provide a robot behavior that can directly respond to external perturbations and human motion. For this, we represent global properties of the robot-human interaction with topology-based coordinates that are computed from arm and torso positions. These coordinates also allow transferring the learned policy to other body shapes and sizes. For training and evaluation, we simulate a dynamic sea rescue scenario and show in quantitative experiments that the policy can solve unseen scenarios with differently shaped humans, floating humans, or with perception noise. Our qualitative experiments show the subsequent transporting after holding is achieved, and we demonstrate that the policy can be directly transferred to a real-world setting.</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/weihao-yuan/>Weihao Yuan</a></span>, <span><a href=/author/kaiyu-hang/>Kaiyu Hang</a></span>, <span><a href=/author/haoran-song/>Haoran Song</a></span>, <span><a href=/author/danica-kragic/>Danica Kragic</a></span>, <span><a href=/author/michael-y.-wang/>Michael Y. Wang</a></span>, <span><a href=/author/johannes-a.-stork/>Johannes A. Stork</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://hangkaiyu.github.io/pdfs/yuan2019a.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/reinforcement-learning-in-topology-based-representation-for-human-body-movement-with-whole-arm-manipulation/cite.bib>Cite</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/energy-gradient-based-graphs-for-planning-within-hand-caging-manipulation/>Energy Gradient-Based Graphs for Planning Within-Hand Caging Manipulation</a></div><a href=/publication/energy-gradient-based-graphs-for-planning-within-hand-caging-manipulation/ class=summary-link><div class=article-style>In this work, we present a within-hand manipulation approach that leverages a simple energy model based on caging grasps made by …</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/walter-g.-bircher/>Walter G. Bircher</a></span>, <span><a href=/author/andrew-s.-morgan/>Andrew S. Morgan</a></span>, <span><a href=/author/kaiyu-hang/>Kaiyu Hang</a></span>, <span><a href=/author/aaron-m.-dollar/>Aaron M. Dollar</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://hangkaiyu.github.io/pdfs/bircher2019a.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/energy-gradient-based-graphs-for-planning-within-hand-caging-manipulation/cite.bib>Cite</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/pre-grasp-sliding-manipulation-of-thin-objects-using-soft-compliant-or-underactuated-hands/>Pre-Grasp Sliding Manipulation of Thin Objects Using Soft, Compliant, or Underactuated Hands</a></div><a href=/publication/pre-grasp-sliding-manipulation-of-thin-objects-using-soft-compliant-or-underactuated-hands/ class=summary-link><div class=article-style>Person re-identification is a critical security task for recognizing a person across spatially disjoint sensors. Previous work can be …</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/kaiyu-hang/>Kaiyu Hang</a></span>, <span><a href=/author/andrew-s.-morgan/>Andrew S. Morgan</a></span>, <span><a href=/author/aaron-m.-dollar/>Aaron M. Dollar</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://hangkaiyu.github.io/pdfs/hang2019a.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/pre-grasp-sliding-manipulation-of-thin-objects-using-soft-compliant-or-underactuated-hands/cite.bib>Cite</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/placing-objects-with-prior-in-hand-manipulation-using-dexterous-manipulation-graphs/>Placing Objects with prior In-Hand Manipulation using Dexterous Manipulation Graphs</a></div><a href=/publication/placing-objects-with-prior-in-hand-manipulation-using-dexterous-manipulation-graphs/ class=summary-link><div class=article-style>Person re-identification is a critical security task for recognizing a person across spatially disjoint sensors. Previous work can be …</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/joshua-a.-haustein/>Joshua A. Haustein</a></span>, <span><a href=/author/silvia-cruciani/>Silvia Cruciani</a></span>, <span><a href=/author/rizwan-asif/>Rizwan Asif</a></span>, <span><a href=/author/kaiyu-hang-and-danica-kragic/>Kaiyu Hang and Danica Kragic</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://hangkaiyu.github.io/pdfs/haustein2019b.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/placing-objects-with-prior-in-hand-manipulation-using-dexterous-manipulation-graphs/cite.bib>Cite</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/dual-arm-in-hand-manipulation-using-visual-feedback/>Dual-Arm In-Hand Manipulation Using Visual Feedback</a></div><a href=/publication/dual-arm-in-hand-manipulation-using-visual-feedback/ class=summary-link><div class=article-style>In this work, we address the problem of executing in-hand manipulation based on visual input. Given an initial grasp, the robot has to …</div></a><div class="stream-meta article-metadata"><div><span><a href=/author/silvia-cruciani/>Silvia Cruciani</a></span>, <span><a href=/author/kaiyu-hang/>Kaiyu Hang</a></span>, <span><a href=/author/christian-smith/>Christian Smith</a></span>, <span><a href=/author/danica-kragic/>Danica Kragic</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://hangkaiyu.github.io/pdfs/cruciani2019a.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/dual-arm-in-hand-manipulation-using-visual-feedback/cite.bib>Cite</a></div></div><div class=ml-3></div></div><nav class=mt-1><ul class="pagination justify-content-center"><li class=page-item><a class=page-link href=/publication-type/1/page/2/>&#187;</a></li></ul></nav></div></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.b4708d4364577c16ab7001b265a063a4.js></script><script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script><script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script><script src=/en/js/wowchemy.min.4922cd6d3d810ab587afa7cdb3851db6.js></script><script src=/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.af9327db0521d4a01354bfc8b77a4324.js type=module></script></body></html>