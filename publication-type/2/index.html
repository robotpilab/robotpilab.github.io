<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><script src=/wowchemy_website/js/mathjax-config.js></script>
<link rel=stylesheet href=/wowchemy_website/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/wowchemy_website/css/wowchemy.73de5f49e1703281b64ee4daa75626f8.css><link rel=stylesheet href=/wowchemy_website/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/wowchemy_website/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=description content="A highly-customizable Hugo research group theme powered by Wowchemy website builder."><link rel=alternate hreflang=en-us href=https://vector-wangel.github.io/wowchemy_website/publication-type/2/><link rel=canonical href=https://vector-wangel.github.io/wowchemy_website/publication-type/2/><link rel=manifest href=/wowchemy_website/manifest.webmanifest><link rel=icon type=image/png href=/wowchemy_website/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/wowchemy_website/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="twitter:image" content="https://vector-wangel.github.io/wowchemy_website/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="Rice Robot$\Pi$ Lab"><meta property="og:url" content="https://vector-wangel.github.io/wowchemy_website/publication-type/2/"><meta property="og:title" content="2 | Rice Robot$\Pi$ Lab"><meta property="og:description" content="A highly-customizable Hugo research group theme powered by Wowchemy website builder."><meta property="og:image" content="https://vector-wangel.github.io/wowchemy_website/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2022-03-04T01:00:00+00:00"><link rel=alternate href=/wowchemy_website/publication-type/2/index.xml type=application/rss+xml title="Rice Robot$\Pi$ Lab"><title>2 | Rice Robot$\Pi$ Lab</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper><script src=/wowchemy_website/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/wowchemy_website/>Rice Robot$\Pi$ Lab</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/wowchemy_website/>Rice Robot$\Pi$ Lab</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/wowchemy_website/project><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/wowchemy_website/post><span>News</span></a></li><li class=nav-item><a class=nav-link href=/wowchemy_website/people><span>People</span></a></li><li class=nav-item><a class=nav-link href=/wowchemy_website/publication><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/wowchemy_website/contact><span>Contact</span></a></li><li class=nav-item><a class=nav-link href=/wowchemy_website/join><span>Join</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class="universal-wrapper pt-3"><h1>2</h1></div><div class=universal-wrapper><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/wowchemy_website/publication/journal-article/>Complex In-Hand Manipulation via Compliance-Enabled Finger Gaiting and Multi-Modal Planning</a></div><a href=/wowchemy_website/publication/journal-article/ class=summary-link><div class=article-style>This work presents a method for achieving complete SO(3) finger gating control of grasped objects against gravity, using a manipulation planner that operates via orthogonal safe modes of a compliant, underactuated hand absent of tactile sensors or joint encoders. The method takes advantage of system compliance to allow the hand to more easily switch contacts while maintaining a stable grasp, and uses a low-latency 6D pose object tracker for online feedback and adaptive recovery from trajectory deviations. The method is demonstrated on a real robot manipulating both convex and non-convex objects, and represents a valuable step towards realizing true robot in-hand manipulation capabilities.</div></a><div class="stream-meta article-metadata"><div><span><a href=/wowchemy_website/author/andrew-s.-morgan/>Andrew S. Morgan</a></span>, <span><a href=/wowchemy_website/author/kaiyu-hang/>Kaiyu Hang</a></span>, <span><a href=/wowchemy_website/author/bowen-wen/>Bowen Wen</a></span>, <span><a href=/wowchemy_website/author/kostas-bekris/>Kostas Bekris</a></span>, <span><a href=/wowchemy_website/author/and-aaron-m.-dollar/>and Aaron M. Dollar</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/2201.07928.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/wowchemy_website/publication/journal-article/cite.bib>Cite</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/wowchemy_website/publication/herding-by-caging-a-formation-based-motion-planning-framework-for-guiding-mobile-agents/>Herding by Caging: A Formation-Based Motion Planning Framework for Guiding Mobile Agents</a></div><a href=/wowchemy_website/publication/herding-by-caging-a-formation-based-motion-planning-framework-for-guiding-mobile-agents/ class=summary-link><div class=article-style>We propose a solution to the problem of herding by caging: given a set of mobile robots (called herders) and a group of moving agents (called sheep), we guide the sheep to a target location without letting them escape from the herders along the way. We model the interaction between the herders and the sheep by defining virtual “repulsive forces” pushing the sheep away from the herders. This enables the herders to partially control the motion of the sheep. We formalize this behavior topologically by applying the notion of caging, a concept used in robotic manipulation. We demonstrate that our approach is provably correct in the sense that the sheep cannot escape from the robots under our assumed motion model. We propose an RRT-based path planning algorithm for herding by caging, demonstrate its probabilistic completeness, and evaluate it in simulations as well as on a group of real mobile robots.</div></a><div class="stream-meta article-metadata"><div><span><a href=/wowchemy_website/author/haoran-song/>Haoran Song</a></span>, <span><a href=/wowchemy_website/author/anastasiia-varava/>Anastasiia Varava</a></span>, <span><a href=/wowchemy_website/author/oleksandr-kravchenko/>Oleksandr Kravchenko</a></span>, <span><a href=/wowchemy_website/author/danica-kragic/>Danica Kragic</a></span>, <span><a href=/wowchemy_website/author/michael-y.-wang/>Michael Y. Wang</a></span>, <span><a href=/wowchemy_website/author/florian-t.-pokorny/>Florian T. Pokorny</a></span>, <span><a href=/wowchemy_website/author/kaiyu-hang/>Kaiyu Hang</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://hangkaiyu.github.io/pdfs/song2021a.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/wowchemy_website/publication/herding-by-caging-a-formation-based-motion-planning-framework-for-guiding-mobile-agents/cite.bib>Cite</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/wowchemy_website/publication/manipulation-for-self-identification-and-self-identification-for-better-manipulation/>Manipulation for self-Identification, and self-Identification for better manipulation</a></div><a href=/wowchemy_website/publication/manipulation-for-self-identification-and-self-identification-for-better-manipulation/ class=summary-link><div class=article-style>The process of modeling a series of hand-object parameters is crucial for precise and controllable robotic in-hand manipulation because it enables the mapping from the hand’s actuation input to the object’s motion to be ob-tained. Without assuming that most of these model parameters are known a priori or can be easily estimated by sensors, we focus on equipping robots with the ability to actively self-identify necessary model parameters using minimal sensing. Here, we derive algorithms, on the basis of the concept of virtual linkage-based representations (VLRs), to self-identify the underlying mechanics of hand-object systems via exploratory manipulation actions and probabilistic reasoning and, in turn, show that the self-identified VLR can enable the control of precise in-hand ma-nipulation. To validate our framework, we instantiated the proposed system on a Yale Model O hand without joint encoders or tactile sensors. The passive adaptability of the underactuated hand greatly facilitates the self-identification process, because they naturally secure stable hand-object interactions during random exploration. Relying solely on an in-hand camera, our system can effectively self-identify the VLRs, even when some fingers are replaced with novel designs. In addition, we show in-hand manipulation applications of handwriting, marble maze playing, and cup stacking to demonstrate the effectiveness of the VLR in precise in-hand manipulation control.</div></a><div class="stream-meta article-metadata"><div><span><a href=/wowchemy_website/author/kaiyu-hang/>Kaiyu Hang</a></span>, <span><a href=/wowchemy_website/author/walter-g.-bircher/>Walter G. Bircher</a></span>, <span><a href=/wowchemy_website/author/andrew-s.-morgan/>Andrew S. Morgan</a></span>, <span><a href=/wowchemy_website/author/aaron-m.-dollar/>Aaron M. Dollar</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://hangkaiyu.github.io/self-identification.html target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/wowchemy_website/publication/manipulation-for-self-identification-and-self-identification-for-better-manipulation/cite.bib>Cite</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/wowchemy_website/publication/benchmarking-in-hand-manipulation/>Benchmarking In-Hand Manipulation</a></div><a href=/wowchemy_website/publication/benchmarking-in-hand-manipulation/ class=summary-link><div class=article-style>The purpose of this benchmark is to evaluate the planning and control aspects of robotic in-hand manipulation systems. The goal is to assess the system&rsquo;s ability to change the position of a handheld object using the fingers, environment, or a combination of both. We provide examples of initial and goal states (i.e., static object poses and fingertip locations) for various in-hand manipulation tasks, given an object surface mesh from the YCB dataset. We further propose metrics that measure the error in reaching the goal state from a specific initial state. When aggregated across all tasks, these metrics also serve as a measure of the system&rsquo;s in-hand manipulation capability. We provide supporting software, task examples, and evaluation results associated with the benchmark.</div></a><div class="stream-meta article-metadata"><div><span><a href=/wowchemy_website/author/silvia-cruciani/>Silvia Cruciani</a></span>, <span><a href=/wowchemy_website/author/balakumar-sundaralingam/>Balakumar Sundaralingam</a></span>, <span><a href=/wowchemy_website/author/kaiyu-hang/>Kaiyu Hang</a></span>, <span><a href=/wowchemy_website/author/vikash-kumar/>Vikash Kumar</a></span>, <span><a href=/wowchemy_website/author/tucker-hermans/>Tucker Hermans</a></span>, <span><a href=/wowchemy_website/author/danica-kragic/>Danica Kragic</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://hangkaiyu.github.io/pdfs/cruciani2020a.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/wowchemy_website/publication/benchmarking-in-hand-manipulation/cite.bib>Cite</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/wowchemy_website/publication/benchmarking-cluttered-robot-pick-and-place-manipulation-with-the-box-and-blocks-test/>Benchmarking Cluttered Robot Pick-and-Place Manipulation with the Box and Blocks Test</a></div><a href=/wowchemy_website/publication/benchmarking-cluttered-robot-pick-and-place-manipulation-with-the-box-and-blocks-test/ class=summary-link><div class=article-style>In this work, we propose a pick-and-place benchmark to assess the manipulation capabilities of a robotic system. The benchmark is based on the Box and Blocks Test (BBT), a task that has been utilized for decades by the rehabilitation community to assess unilateral gross manual dexterity in humans. We propose three robot benchmarking protocols in this work that hold true to the spirit of the original clinical tests: the Modified-BBT, the Targeted-BBT, and the Standard-BBT. These protocols can be implemented by the greater robotics research community, as the physical BBT setup has been widely distributed with the Yale-CMU-Berkeley (YCB) Object and Model Set. The difficulty of the three protocols increases sequentially, adding a new performance component at each level, and therefore aiming to assess various aspects of the system separately. Clinical task-time norms are summarized for able-bodied human participants. We provide baselines for all three protocols with off-the-shelf planning and perception algorithms on a Barrett WAM and a Franka Emika Panda manipulator and compare the results with human performance.</div></a><div class="stream-meta article-metadata"><div><span><a href=/wowchemy_website/author/andrew-s.-morgan/>Andrew S. Morgan</a></span>, <span><a href=/wowchemy_website/author/kaiyu-hang/>Kaiyu Hang</a></span>, <span><a href=/wowchemy_website/author/walter-g.-bircher/>Walter G. Bircher</a></span>, <span><a href=/wowchemy_website/author/fadi-m.-alladkani/>Fadi M. Alladkani</a></span>, <span><a href=/wowchemy_website/author/abhinav-gandhi/>Abhinav Gandhi</a></span>, <span><a href=/wowchemy_website/author/berk-calli/>Berk Calli</a></span>, <span><a href=/wowchemy_website/author/aaron-m.-dollar/>Aaron M. Dollar</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://hangkaiyu.github.io/pdfs/morgan2020a.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/wowchemy_website/publication/benchmarking-cluttered-robot-pick-and-place-manipulation-with-the-box-and-blocks-test/cite.bib>Cite</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/wowchemy_website/publication/calculating-the-support-function-of-complex-continuous-surfaces-with-applications-to-minimum-distance-computation-and-optimal-grasp-planning/>Calculating the Support Function of Complex Continuous Surfaces With Applications to Minimum Distance Computation and Optimal Grasp Planning</a></div><a href=/wowchemy_website/publication/calculating-the-support-function-of-complex-continuous-surfaces-with-applications-to-minimum-distance-computation-and-optimal-grasp-planning/ class=summary-link><div class=article-style>&ldquo;The support function of a surface&rdquo; is a fundamental concept in mathematics and a crucial operation for algorithms in robotics, such as those for collision detection and grasp planning. It is possible to calculate the support function of a convex body in closed form. However, for complex continuous surfaces, especially non-convex ones, this calculation can be far more difficult, and no general solution is available so far. This limits the applicability of related algorithms. This paper presents a branch-and-bound (B&amp;B) algorithm to calculate the support function of complex continuous surfaces. An upper bound of the support function over a surface domain is derived. While a surface domain is divided into subdomains, the upper bound of the support function over any subdomain is proved to be not greater than the one over the original domain. Then, as the B&amp;B algorithm sequentially divides the surface domain by dividing its subdomain having a greater upper bound than the others, the maximum upper bound over all subdomains is monotonically decreasing and converges to the exact value of the desired support function. Furthermore, with the aid of the B&amp;B algorithm, this paper derives new algorithms for the minimum distance between complex continuous surfaces and for globally optimal grasps on objects with continuous surfaces. A number of numerical examples are provided to demonstrate the effectiveness of the proposed algorithms.</div></a><div class="stream-meta article-metadata"><div><span><a href=/wowchemy_website/author/yu-zheng/>Yu Zheng</a></span>, <span><a href=/wowchemy_website/author/kaiyu-hang/>Kaiyu Hang</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://hangkaiyu.github.io/pdfs/zheng2020a.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/wowchemy_website/publication/calculating-the-support-function-of-complex-continuous-surfaces-with-applications-to-minimum-distance-computation-and-optimal-grasp-planning/cite.bib>Cite</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/wowchemy_website/publication/object-agnostic-dexterous-manipulation-of-partially-constrained-trajectories/>Object-Agnostic Dexterous Manipulation of Partially Constrained Trajectories</a></div><a href=/wowchemy_website/publication/object-agnostic-dexterous-manipulation-of-partially-constrained-trajectories/ class=summary-link><div class=article-style>We address the problem of controlling a partially constrained trajectory of the manipulation frame - an arbitrary frame of reference rigidly attached to the object - as the desired motion about this frame is often underdefined. This may be apparent, for example, when the task requires control only about the translational dimensions of the manipulation frame, with disregard to the rotational dimensions. This scenario complicates the computation of the grasp frame trajectory, as the mobility of the mechanism is likely limited due to the constraints imposed by the closed kinematic chain. In this letter, we address this problem by combining a learned, object-agnostic manipulation model of the gripper with Model Predictive Control (MPC). This combination facilitates an approach to simple vision-based control of robotic hands with generalized models, enabling a single manipulation model to extend to different task requirements. By tracking the hand-object configuration through vision, the proposed framework is able to accurately control the trajectory of the manipulation frame along translational, rotational, or mixed trajectories. We provide experiments quantifying the utility of this framework, analyzing its ability to control different objects over varied horizon lengths and optimization iterations, and finally, we implement the controller on a physical system.</div></a><div class="stream-meta article-metadata"><div><span><a href=/wowchemy_website/author/andrew-s.-morgan/>Andrew S. Morgan</a></span>, <span><a href=/wowchemy_website/author/kaiyu-hang/>Kaiyu Hang</a></span>, <span><a href=/wowchemy_website/author/aaron-m.-dollar/>Aaron M. Dollar</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://hangkaiyu.github.io/pdfs/morgan2020b.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/wowchemy_website/publication/object-agnostic-dexterous-manipulation-of-partially-constrained-trajectories/cite.bib>Cite</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/wowchemy_website/publication/end-to-end-nonprehensile-rearrangement-with-deep-reinforcement-learning-and-simulation-to-reality-transfer/>End-to-End Nonprehensile Rearrangement with Deep Reinforcement Learning and Simulation-to-Reality Transfer</a></div><a href=/wowchemy_website/publication/end-to-end-nonprehensile-rearrangement-with-deep-reinforcement-learning-and-simulation-to-reality-transfer/ class=summary-link><div class=article-style>Nonprehensile rearrangement is the problem of controlling a robot to interact with objects through pushing actions in order to reconfigure the objects into a predefined goal pose. In this work, we rearrange one object at a time in an environment with obstacles using an end-to-end policy that maps raw pixels as visual input to control actions without any form of engineered feature extraction. To reduce the amount of training data that needs to be collected using a real robot, we propose a simulation-to-reality transfer approach. In the first step, we model the nonprehensile rearrangement task in simulation and use deep reinforcement learning to learn a suitable rearrangement policy, which requires hundreds of thousands of example actions for training. Thereafter, we collect a small dataset of only 70 episodes of real-world actions as supervised examples for adapting the learned rearrangement policy to real-world input data. In this process, we make use of newly proposed strategies for improving the reinforcement learning process, such as heuristic exploration and the curation of a balanced set of experiences. We evaluate our method in both simulation and real settings using a Baxter robot to show that the proposed approach can effectively improve the training process in simulation, as well as efficiently adapt the learned policy to the real-world application, even when the camera pose is different from simulation. Additionally, we show that the learned system can not only provide adaptive behavior to handle unforeseen events during executions, such as distracting objects, sudden changes in the positions of the objects, and obstacles, but also can deal with obstacle shapes that were not present in the training process.</div></a><div class="stream-meta article-metadata"><div><span><a href=/wowchemy_website/author/weihao-yuan/>Weihao Yuan</a></span>, <span><a href=/wowchemy_website/author/kaiyu-hang/>Kaiyu Hang</a></span>, <span><a href=/wowchemy_website/author/danica-kragic/>Danica Kragic</a></span>, <span><a href=/wowchemy_website/author/michael-y.-wang/>Michael Y. Wang</a></span>, <span><a href=/wowchemy_website/author/johannes-a.-stork/>Johannes A. Stork</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://hangkaiyu.github.io/pdfs/yuan2019b.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/wowchemy_website/publication/end-to-end-nonprehensile-rearrangement-with-deep-reinforcement-learning-and-simulation-to-reality-transfer/cite.bib>Cite</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/wowchemy_website/publication/pre-grasp-sliding/>Pre-Grasp Sliding Manipulation of Thin Objects Using Soft, Compliant, or Underactuated Hands</a></div><a href=/wowchemy_website/publication/pre-grasp-sliding/ class=summary-link><div class=article-style>We address the problem of pre-grasp sliding manipulation, which is an essential skill when a thin object cannot be directly grasped from a flat surface. Leveraging the passive reconfigurability of soft, compliant, or underactuated robotic hands, we formulate this problem as an integrated motion and grasp planning problem and plan the manipulation directly in the robot configuration space. Instead of explicitly precomputing a pair of valid start and goal configurations and then planning a path to connect them in a separate step, our planner actively samples start and goal robot configurations from configuration sampleable regions modeled from the geometries of the object and support surface. While randomly connecting the sampled start and goal configurations in pairs, the planner verifies whether any connected pair can achieve the task to finally confirm a solution. The proposed planner is implemented and evaluated both in simulation and on a real robot. Given the inherent compliance of the employed Yale T42 hand, we relax the motion constraints and show that the planning performance is significantly boosted. Moreover, we show that our planner outperforms two baseline planners and that it can deal with objects and support surfaces of arbitrary geometries and sizes.</div></a><div class="stream-meta article-metadata"><div><span><a href=/wowchemy_website/author/kaiyu-hang/>Kaiyu Hang</a></span>, <span><a href=/wowchemy_website/author/andrew-s.-morgan/>Andrew S. Morgan</a></span>, <span><a href=/wowchemy_website/author/aaron-m.-dollar/>Aaron M. Dollar</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://hangkaiyu.github.io/pdfs/hang2019a.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/wowchemy_website/publication/pre-grasp-sliding/cite.bib>Cite</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/wowchemy_website/publication/hand-object-configuration-estimation-using-particle-filters-for-dexterous-in-hand-manipulation/>Hand-object configuration estimation using particle filters for dexterous in-hand manipulation</a></div><a href=/wowchemy_website/publication/hand-object-configuration-estimation-using-particle-filters-for-dexterous-in-hand-manipulation/ class=summary-link><div class=article-style>We consider the problem of dexterous manipulation with a focus on unknown or uncertain hand-object parameters, such as hand configuration, object pose within the hand, and contact positions. In this work, we formulate a generic framework for hand-object configuration estimation using underactuated hands as an example. Due to the passive reconfigurability and lack of encoders in the hand&rsquo;s joints, it is challenging to estimate, plan, and actively control underactuated manipulation. By modeling the grasp constraints, we present a particle filter-based framework to estimate the hand configuration. Specifically, given an arbitrary grasp, we start by sampling a set of hand configuration hypotheses and then randomly manipulate the object within the hand. While observing the object&rsquo;s movements as evidence using an external camera, which is not necessarily calibrated with the hand frame, our estimator calculates the likelihood of each hypothesis to iteratively estimate the hand configuration. Once converged, the estimator is used to track the hand configuration in real-time for future manipulations. Thereafter, we develop an algorithm to precisely plan and control the underactuated manipulation to move the grasped object to desired poses. In contrast to most other dexterous manipulation approaches, our framework does not require any tactile sensing or joint encoders and can directly operate on any novel objects without requiring a model of the object a priori. We implemented our framework on both the Yale Model O hand and the Yale T42 hand. The results show that the estimation is accurate for different objects and that the framework can be easily adapted across different underactuated hand models. Finally, we evaluated our planning and control algorithm with handwriting tasks and demonstrated the effectiveness of the proposed framework.</div></a><div class="stream-meta article-metadata"><div><span><a href=/wowchemy_website/author/kaiyu-hang/>Kaiyu Hang</a></span>, <span><a href=/wowchemy_website/author/walter-g.-bircher/>Walter G. Bircher</a></span>, <span><a href=/wowchemy_website/author/andrew-s.-morgan/>Andrew S. Morgan</a></span>, <span><a href=/wowchemy_website/author/and-aaron-m.-dollar/>and Aaron M. Dollar</a></span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://hangkaiyu.github.io/pdfs/hang2019c.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/wowchemy_website/publication/hand-object-configuration-estimation-using-particle-filters-for-dexterous-in-hand-manipulation/cite.bib>Cite</a></div></div><div class=ml-3></div></div><nav class=mt-1><ul class="pagination justify-content-center"><li class=page-item><a class=page-link href=/wowchemy_website/publication-type/2/page/2/>&#187;</a></li></ul></nav></div></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/wowchemy_website/js/vendor-bundle.min.32ee83730ed883becad04bc5170512cc.js></script>
<script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/wowchemy_website/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script>
<script src=/wowchemy_website/en/js/wowchemy.min.53e2d4aa5b5aee6f858cf53dc1025ec4.js></script>
<script src=/wowchemy_website/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/wowchemy_website/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>